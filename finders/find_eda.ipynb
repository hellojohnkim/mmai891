{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find EDA\n",
    "\n",
    "- Stephen W. Thomas\n",
    "\n",
    "This script does a lot of common EDA (word count, phrase count, etc.) on a given corpus and outputs the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['s', 'rt', 'br'])\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "\n",
    "\n",
    "# Simple preprocessor.\n",
    "# Note that this function will be called on each document before stop words are \n",
    "# removed, before lowercases, and before tokenization. I.e., the raw documents go into this function.\n",
    "def preprocessor(doc):\n",
    "\n",
    "    doc = doc.replace(u'’', u\"'\")\n",
    "    doc = doc.replace(u'“', u'\"')\n",
    "    doc = doc.replace(u'”', u'\"')\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    doc = BeautifulSoup(doc, \"lxml\").get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    \n",
    "    # remove URLS like pic.twitter.com/SODA\n",
    "    doc = re.sub(r'\\b\\S*(\\.com|\\.edu|\\.net|\\.gov|\\.ca|\\.org)(/\\S*)?', '', doc)\n",
    "    \n",
    "    # Make strings like \"@ DrJoe\" become \"@DrJoe\"\n",
    "    doc = re.sub(r'(\\@)(\\s+)(.)', r'\\1\\3', doc)\n",
    "    \n",
    "    # Make strings like \"# DrJoe\" become \"#DrJoe\"\n",
    "    doc = re.sub(r'(#)(\\s+)(.)', r'\\1\\3', doc)\n",
    "    \n",
    "    doc = expandContractions(doc)\n",
    "    \n",
    "    #spacy_doc = nlp(doc)\n",
    "    #doc = \" \".join([token.lemma_ for token in spacy_doc])\n",
    "    \n",
    "    #Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    doc = ' '.join([w for w in doc.split() if w not in stop_words])\n",
    "    \n",
    "    doc = ' '.join([lemmer.lemmatize(w) for w in doc.split()])\n",
    "    return doc\n",
    "\n",
    "def find_counts(data, text_col = \"tweet_text\", min_df=2, ngrams=(1,1)):   \n",
    "    \n",
    "    # Quick and dirty counter of terms and tokens (before we whittle down later)\n",
    "    results = Counter()\n",
    "    data_pre = data.apply(preprocessor)\n",
    "    data_pre.str.split().apply(results.update)\n",
    "    \n",
    "    n_docs = data.shape[0]\n",
    "    n_terms = len(results)\n",
    "    n_tokens = sum(results.values())\n",
    "    \n",
    "    print('Number of documents: {}'.format(n_docs))\n",
    "    print('Number of word forms (terms): {}'.format(n_terms))\n",
    "    print('Number of words (tokens): {}'.format(n_tokens))\n",
    "    print('Mean words per document: {:.1f}'.format(n_tokens / n_docs))\n",
    "    print('Mean term occurance: {:.1f}'.format(np.mean(list(results.values()))))\n",
    "    for m in [1, 5, 10, 100]:\n",
    "        vs = {k:v for (k, v) in results.items() if v <= m}\n",
    "        print('Number (Pct) of terms occuring <= {}: {} ({:.1f})'.format(m, len(vs), 100*len(vs)/n_terms))\n",
    "        \n",
    "    \n",
    "    # We override the token_pattern in order to keep @signs and #hashtags\n",
    "    vec = CountVectorizer(      preprocessor=preprocessor,\n",
    "                                token_pattern = '[a-zA-Z0-9@#]+',\n",
    "                                stop_words=stop_words,\n",
    "                                lowercase=True,\n",
    "                                min_df=min_df,\n",
    "                                ngram_range=ngrams,\n",
    "                                max_features=10000)\n",
    "    \n",
    "    bow = vec.fit_transform(data)\n",
    "    vocab = vec.get_feature_names()\n",
    "    tdm = pd.DataFrame(bow.toarray(), columns=vocab)\n",
    "        \n",
    "    \n",
    "    n_tokens = sum(tdm.sum())\n",
    "    n_docs = tdm.shape[0]\n",
    "    phrases = list(tdm.columns)\n",
    "    counts = pd.DataFrame(data={'Phrase': phrases, \n",
    "                                'Characters': [len(x) for x in phrases],\n",
    "                                'Terms': [x.count(' ')+1 for x in phrases],\n",
    "                                'Count': tdm.sum(),\n",
    "                                'Count Pct': tdm.sum() / n_tokens,\n",
    "                                'Docs': tdm.astype(bool).sum(),\n",
    "                                'Docs Pct': tdm.astype(bool).sum() / n_docs,\n",
    "                          })\n",
    "    \n",
    "    counts = counts.sort_values(by=['Count'], ascending=False)\n",
    "    \n",
    "    print('Top {} words:'.format(num_words_to_print))\n",
    "    print(counts.head(num_words_to_print))\n",
    "    print('\\nBottom {} words:'.format(num_words_to_print))\n",
    "    print(counts.tail(num_words_to_print))\n",
    "    \n",
    "    \n",
    "    return tdm, vocab, counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_to_print=25\n",
    "\n",
    "def do_it(file_base, id_col=\"id\", text_col=\"tweet_text\", target_cols=[]):\n",
    "    df = pd.read_csv('../data/'+file_base+'.csv')\n",
    "    \n",
    "    # Drop rows without any text\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    \n",
    "    #print(\"\\nUnigrams\")\n",
    "    tdm, vocab, counts = find_counts(df[text_col], min_df=10, ngrams=(1,3))\n",
    "    tdm.to_csv('out/'+file_base+'_tdm.csv', index=False)\n",
    "    counts.to_csv('out/'+file_base+'_counts.csv', index=False)\n",
    "    \n",
    "    \n",
    "    # Now, lets compute the counts and percentages per target level\n",
    "    for target_col in target_cols:\n",
    "        tdm_tmp = tdm\n",
    "        col_name = 'target_'+target_col\n",
    "        tdm_tmp[col_name] = df[target_col]\n",
    "        s = tdm_tmp.groupby(by=[col_name]).sum().T\n",
    "        n_levels=s.shape[1]\n",
    "        s['Total'] = s.sum(axis=1)\n",
    "        for i in range(0, n_levels):\n",
    "            new_col_name = str(s.columns[i]) + ' Pct'\n",
    "            s[new_col_name] = s.iloc[:,i] / s['Total']\n",
    "        s.to_csv('out/'+file_base+'_counts_'+target_col+'.csv', index=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4694\n",
      "Number of word forms (terms): 12584\n",
      "Number of words (tokens): 72235\n",
      "Mean words per document: 15.4\n",
      "Mean term occurance: 5.7\n",
      "Number (Pct) of terms occuring <= 1: 6967 (55.4)\n",
      "Number (Pct) of terms occuring <= 5: 10549 (83.8)\n",
      "Number (Pct) of terms occuring <= 10: 11400 (90.6)\n",
      "Number (Pct) of terms occuring <= 100: 12497 (99.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                    Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "president        president           9      1   1093   0.017811   986   \n",
      "trump                trump           5      1    981   0.015986   916   \n",
      "biden                biden           5      1    564   0.009191   550   \n",
      "american          american           8      1    560   0.009126   520   \n",
      "donald              donald           6      1    555   0.009044   549   \n",
      "donald trump  donald trump          12      2    546   0.008898   540   \n",
      "need                  need           4      1    526   0.008572   468   \n",
      "country            country           7      1    462   0.007529   450   \n",
      "vp                      vp           2      1    451   0.007349   450   \n",
      "today                today           5      1    426   0.006942   416   \n",
      "day                    day           3      1    418   0.006812   384   \n",
      "nation              nation           6      1    409   0.006665   394   \n",
      "people              people           6      1    403   0.006567   377   \n",
      "make                  make           4      1    403   0.006567   378   \n",
      "time                  time           4      1    382   0.006225   361   \n",
      "america            america           7      1    374   0.006095   365   \n",
      "health              health           6      1    338   0.005508   283   \n",
      "care                  care           4      1    313   0.005101   253   \n",
      "year                  year           4      1    311   0.005068   292   \n",
      "work                  work           4      1    293   0.004775   270   \n",
      "vp biden          vp biden           8      2    290   0.004726   290   \n",
      "just                  just           4      1    279   0.004547   269   \n",
      "right                right           5      1    249   0.004058   208   \n",
      "know                  know           4      1    239   0.003895   221   \n",
      "middle              middle           6      1    233   0.003797   223   \n",
      "\n",
      "              Docs Pct  \n",
      "president     0.210055  \n",
      "trump         0.195143  \n",
      "biden         0.117171  \n",
      "american      0.110780  \n",
      "donald        0.116958  \n",
      "donald trump  0.115040  \n",
      "need          0.099702  \n",
      "country       0.095867  \n",
      "vp            0.095867  \n",
      "today         0.088624  \n",
      "day           0.081807  \n",
      "nation        0.083937  \n",
      "people        0.080315  \n",
      "make          0.080528  \n",
      "time          0.076907  \n",
      "america       0.077759  \n",
      "health        0.060290  \n",
      "care          0.053899  \n",
      "year          0.062207  \n",
      "work          0.057520  \n",
      "vp biden      0.061781  \n",
      "just          0.057307  \n",
      "right         0.044312  \n",
      "know          0.047081  \n",
      "middle        0.047507  \n",
      "\n",
      "Bottom 25 words:\n",
      "                                      Phrase  Characters  Terms  Count  \\\n",
      "corner country                corner country          14      2     10   \n",
      "vital                                  vital           5      1     10   \n",
      "contract                            contract           8      1     10   \n",
      "parents income                parents income          14      2     10   \n",
      "spoke                                  spoke           5      1     10   \n",
      "lack                                    lack           4      1     10   \n",
      "staff                                  staff           5      1     10   \n",
      "nuestra                              nuestra           7      1     10   \n",
      "known                                  known           5      1     10   \n",
      "crisis head                      crisis head          11      2     10   \n",
      "donald trump year          donald trump year          17      3     10   \n",
      "crisis american              crisis american          15      2     10   \n",
      "officially                        officially          10      1     10   \n",
      "online                                online           6      1     10   \n",
      "advance                              advance           7      1     10   \n",
      "betsy                                  betsy           5      1     10   \n",
      "op                                        op           2      1     10   \n",
      "stand @nra                        stand @nra          10      2     10   \n",
      "joke                                    joke           4      1     10   \n",
      "join campaign                  join campaign          13      2     10   \n",
      "joe photo day                  joe photo day          13      3     10   \n",
      "stage tonight                  stage tonight          13      2     10   \n",
      "biden president obama  biden president obama          21      3     10   \n",
      "university                        university          10      1     10   \n",
      "hurt                                    hurt           4      1     10   \n",
      "\n",
      "                       Count Pct  Docs  Docs Pct  \n",
      "corner country          0.000163    10   0.00213  \n",
      "vital                   0.000163    10   0.00213  \n",
      "contract                0.000163    10   0.00213  \n",
      "parents income          0.000163    10   0.00213  \n",
      "spoke                   0.000163    10   0.00213  \n",
      "lack                    0.000163    10   0.00213  \n",
      "staff                   0.000163    10   0.00213  \n",
      "nuestra                 0.000163    10   0.00213  \n",
      "known                   0.000163    10   0.00213  \n",
      "crisis head             0.000163    10   0.00213  \n",
      "donald trump year       0.000163    10   0.00213  \n",
      "crisis american         0.000163    10   0.00213  \n",
      "officially              0.000163    10   0.00213  \n",
      "online                  0.000163    10   0.00213  \n",
      "advance                 0.000163    10   0.00213  \n",
      "betsy                   0.000163    10   0.00213  \n",
      "op                      0.000163    10   0.00213  \n",
      "stand @nra              0.000163    10   0.00213  \n",
      "joke                    0.000163    10   0.00213  \n",
      "join campaign           0.000163    10   0.00213  \n",
      "joe photo day           0.000163    10   0.00213  \n",
      "stage tonight           0.000163    10   0.00213  \n",
      "biden president obama   0.000163    10   0.00213  \n",
      "university              0.000163    10   0.00213  \n",
      "hurt                    0.000163    10   0.00213  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"JoeBidenTweets\", id_col=\"id\", text_col=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4999\n",
      "Number of word forms (terms): 32278\n",
      "Number of words (tokens): 219633\n",
      "Mean words per document: 43.9\n",
      "Mean term occurance: 6.8\n",
      "Number (Pct) of terms occuring <= 1: 20622 (63.9)\n",
      "Number (Pct) of terms occuring <= 5: 27813 (86.2)\n",
      "Number (Pct) of terms occuring <= 10: 29511 (91.4)\n",
      "Number (Pct) of terms occuring <= 100: 31928 (98.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "              Phrase  Characters  Terms  Count  Count Pct  Docs  Docs Pct\n",
      "like            like           4      1   3146   0.015443  1955  0.391078\n",
      "taste          taste           5      1   2819   0.013838  1889  0.377876\n",
      "good            good           4      1   2453   0.012041  1785  0.357071\n",
      "flavor        flavor           6      1   2378   0.011673  1556  0.311262\n",
      "coffee        coffee           6      1   1987   0.009754   737  0.147429\n",
      "just            just           4      1   1854   0.009101  1383  0.276655\n",
      "tea              tea           3      1   1585   0.007780   508  0.101620\n",
      "product      product           7      1   1492   0.007324  1018  0.203641\n",
      "great          great           5      1   1463   0.007181  1198  0.239648\n",
      "really        really           6      1   1301   0.006386   985  0.197039\n",
      "make            make           4      1   1277   0.006268  1004  0.200840\n",
      "love            love           4      1   1190   0.005841   945  0.189038\n",
      "little        little           6      1   1011   0.004963   791  0.158232\n",
      "use              use           3      1    995   0.004884   761  0.152230\n",
      "chocolate  chocolate           9      1    973   0.004776   466  0.093219\n",
      "cup              cup           3      1    948   0.004653   525  0.105021\n",
      "sugar          sugar           5      1    943   0.004629   587  0.117423\n",
      "did              did           3      1    909   0.004462   677  0.135427\n",
      "drink          drink           5      1    878   0.004310   507  0.101420\n",
      "sweet          sweet           5      1    863   0.004236   633  0.126625\n",
      "water          water           5      1    849   0.004168   485  0.097019\n",
      "time            time           4      1    777   0.003814   650  0.130026\n",
      "better        better           6      1    768   0.003770   657  0.131426\n",
      "bag              bag           3      1    745   0.003657   465  0.093019\n",
      "try              try           3      1    717   0.003520   626  0.125225\n",
      "\n",
      "Bottom 25 words:\n",
      "                            Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "instructions          instructions          12      1     10   0.000049    10   \n",
      "cranberries            cranberries          11      1     10   0.000049    10   \n",
      "little hard            little hard          11      2     10   0.000049    10   \n",
      "flavor add              flavor add          10      2     10   0.000049    10   \n",
      "little chocolate  little chocolate          16      2     10   0.000049    10   \n",
      "gary peterson        gary peterson          13      2     10   0.000049    10   \n",
      "fixed                        fixed           5      1     10   0.000049    10   \n",
      "crowd                        crowd           5      1     10   0.000049    10   \n",
      "crumble                    crumble           7      1     10   0.000049    10   \n",
      "airy                          airy           4      1     10   0.000049    10   \n",
      "crunchier                crunchier           9      1     10   0.000049    10   \n",
      "price reasonable  price reasonable          16      2     10   0.000049    10   \n",
      "price point            price point          11      2     10   0.000049    10   \n",
      "cup day                    cup day           7      2     10   0.000049    10   \n",
      "bit stronger          bit stronger          12      2     10   0.000049    10   \n",
      "gary                          gary           4      1     10   0.000049    10   \n",
      "cupboard                  cupboard           8      1     10   0.000049    10   \n",
      "steeping                  steeping           8      1     10   0.000049    10   \n",
      "childhood                childhood           9      1     10   0.000049    10   \n",
      "measured                  measured           8      1     10   0.000049    10   \n",
      "resist                      resist           6      1     10   0.000049    10   \n",
      "chia seeds              chia seeds          10      2     10   0.000049    10   \n",
      "free diet                free diet           9      2     10   0.000049    10   \n",
      "started eating      started eating          14      2     10   0.000049    10   \n",
      "smooth flavor        smooth flavor          13      2     10   0.000049    10   \n",
      "\n",
      "                  Docs Pct  \n",
      "instructions         0.002  \n",
      "cranberries          0.002  \n",
      "little hard          0.002  \n",
      "flavor add           0.002  \n",
      "little chocolate     0.002  \n",
      "gary peterson        0.002  \n",
      "fixed                0.002  \n",
      "crowd                0.002  \n",
      "crumble              0.002  \n",
      "airy                 0.002  \n",
      "crunchier            0.002  \n",
      "price reasonable     0.002  \n",
      "price point          0.002  \n",
      "cup day              0.002  \n",
      "bit stronger         0.002  \n",
      "gary                 0.002  \n",
      "cupboard             0.002  \n",
      "steeping             0.002  \n",
      "childhood            0.002  \n",
      "measured             0.002  \n",
      "resist               0.002  \n",
      "chia seeds           0.002  \n",
      "free diet            0.002  \n",
      "started eating       0.002  \n",
      "smooth flavor        0.002  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"amazon_food_reviews_10\", id_col=\"reviewID\", text_col=\"reviewText\", target_cols=['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10000\n",
      "Number of word forms (terms): 144168\n",
      "Number of words (tokens): 1122017\n",
      "Mean words per document: 112.2\n",
      "Mean term occurance: 7.8\n",
      "Number (Pct) of terms occuring <= 1: 92320 (64.0)\n",
      "Number (Pct) of terms occuring <= 5: 125007 (86.7)\n",
      "Number (Pct) of terms occuring <= 10: 132626 (92.0)\n",
      "Number (Pct) of terms occuring <= 100: 142569 (98.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "              Phrase  Characters  Terms  Count  Count Pct  Docs  Docs Pct\n",
      "movie          movie           5      1  19524   0.019180  6383    0.6383\n",
      "film            film           4      1  18016   0.017699  5796    0.5796\n",
      "like            like           4      1   8149   0.008006  4671    0.4671\n",
      "just            just           4      1   6861   0.006740  4128    0.4128\n",
      "good            good           4      1   5928   0.005824  3769    0.3769\n",
      "time            time           4      1   5755   0.005654  3840    0.3840\n",
      "story          story           5      1   4978   0.004890  3135    0.3135\n",
      "make            make           4      1   4847   0.004762  3328    0.3328\n",
      "character  character           9      1   4758   0.004674  2927    0.2927\n",
      "really        really           6      1   4545   0.004465  2953    0.2953\n",
      "did              did           3      1   4187   0.004113  2805    0.2805\n",
      "doe              doe           3      1   3894   0.003825  2709    0.2709\n",
      "scene          scene           5      1   3812   0.003745  2418    0.2418\n",
      "great          great           5      1   3710   0.003645  2549    0.2549\n",
      "bad              bad           3      1   3581   0.003518  2347    0.2347\n",
      "people        people           6      1   3575   0.003512  2441    0.2441\n",
      "way              way           3      1   3367   0.003308  2453    0.2453\n",
      "thing          thing           5      1   3066   0.003012  2246    0.2246\n",
      "life            life           4      1   2968   0.002916  2004    0.2004\n",
      "think          think           5      1   2960   0.002908  2224    0.2224\n",
      "watch          watch           5      1   2896   0.002845  2216    0.2216\n",
      "know            know           4      1   2820   0.002770  2111    0.2111\n",
      "love            love           4      1   2778   0.002729  1885    0.1885\n",
      "plot            plot           4      1   2753   0.002705  2067    0.2067\n",
      "seen            seen           4      1   2626   0.002580  2077    0.2077\n",
      "\n",
      "Bottom 25 words:\n",
      "                        Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "corridor              corridor           8      1     16   0.000016    14   \n",
      "lam                        lam           3      1     16   0.000016    10   \n",
      "knowles                knowles           7      1     16   0.000016    10   \n",
      "know make            know make           9      2     16   0.000016    16   \n",
      "rock roll            rock roll           9      2     16   0.000016    14   \n",
      "think make          think make          10      2     16   0.000016    16   \n",
      "watching tv        watching tv          11      2     16   0.000016    16   \n",
      "dam                        dam           3      1     16   0.000016    11   \n",
      "thelma                  thelma           6      1     16   0.000016    13   \n",
      "myrna                    myrna           5      1     16   0.000016    13   \n",
      "way people          way people          10      2     16   0.000016    16   \n",
      "amazing job        amazing job          11      2     16   0.000016    15   \n",
      "decade later      decade later          12      2     16   0.000016    16   \n",
      "december              december           8      1     16   0.000016    16   \n",
      "rusty                    rusty           5      1     16   0.000016    14   \n",
      "jerky                    jerky           5      1     16   0.000016    16   \n",
      "need help            need help           9      2     16   0.000016    16   \n",
      "salvage                salvage           7      1     16   0.000016    14   \n",
      "satanic                satanic           7      1     16   0.000016    14   \n",
      "save life            save life           9      2     16   0.000016    16   \n",
      "inter                    inter           5      1     16   0.000016    16   \n",
      "bickering            bickering           9      1     16   0.000016    16   \n",
      "did believe        did believe          11      2     16   0.000016    16   \n",
      "ness                      ness           4      1     16   0.000016    12   \n",
      "commit suicide  commit suicide          14      2     16   0.000016    15   \n",
      "\n",
      "                Docs Pct  \n",
      "corridor          0.0014  \n",
      "lam               0.0010  \n",
      "knowles           0.0010  \n",
      "know make         0.0016  \n",
      "rock roll         0.0014  \n",
      "think make        0.0016  \n",
      "watching tv       0.0016  \n",
      "dam               0.0011  \n",
      "thelma            0.0013  \n",
      "myrna             0.0013  \n",
      "way people        0.0016  \n",
      "amazing job       0.0015  \n",
      "decade later      0.0016  \n",
      "december          0.0016  \n",
      "rusty             0.0014  \n",
      "jerky             0.0016  \n",
      "need help         0.0016  \n",
      "salvage           0.0014  \n",
      "satanic           0.0014  \n",
      "save life         0.0016  \n",
      "inter             0.0016  \n",
      "bickering         0.0016  \n",
      "did believe       0.0016  \n",
      "ness              0.0012  \n",
      "commit suicide    0.0015  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"imdb.small\", id_col=\"id\", text_col=\"en\", target_cols=['rating', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 6802\n",
      "Number of word forms (terms): 29789\n",
      "Number of words (tokens): 510920\n",
      "Mean words per document: 75.1\n",
      "Mean term occurance: 17.2\n",
      "Number (Pct) of terms occuring <= 1: 14088 (47.3)\n",
      "Number (Pct) of terms occuring <= 5: 22758 (76.4)\n",
      "Number (Pct) of terms occuring <= 10: 25169 (84.5)\n",
      "Number (Pct) of terms occuring <= 100: 28957 (97.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "              Phrase  Characters  Terms  Count  Count Pct  Docs  Docs Pct\n",
      "business    business           8      1  13074   0.019000  5392  0.792708\n",
      "loan            loan           4      1  11374   0.016530  6090  0.895325\n",
      "year            year           4      1   6938   0.010083  4618  0.678918\n",
      "sell            sell           4      1   5008   0.007278  3460  0.508674\n",
      "school        school           6      1   4744   0.006894  3018  0.443693\n",
      "buy              buy           3      1   4426   0.006432  3238  0.476036\n",
      "children    children           8      1   4270   0.006206  3520  0.517495\n",
      "old              old           3      1   4129   0.006001  3683  0.541458\n",
      "family        family           6      1   4114   0.005979  2917  0.428844\n",
      "child          child           5      1   4068   0.005912  3002  0.441341\n",
      "group          group           5      1   3856   0.005604  1820  0.267568\n",
      "year old    year old           8      2   3849   0.005594  3554  0.522493\n",
      "able            able           4      1   3811   0.005539  2713  0.398853\n",
      "small          small           5      1   2892   0.004203  2217  0.325934\n",
      "husband      husband           7      1   2867   0.004167  2026  0.297854\n",
      "help            help           4      1   2841   0.004129  2215  0.325640\n",
      "work            work           4      1   2822   0.004101  2019  0.296824\n",
      "income        income           6      1   2737   0.003978  2094  0.307851\n",
      "married      married           7      1   2694   0.003915  2555  0.375625\n",
      "life            life           4      1   2656   0.003860  2172  0.319318\n",
      "selling      selling           7      1   2447   0.003556  2021  0.297118\n",
      "purchase    purchase           8      1   2413   0.003507  1874  0.275507\n",
      "community  community           9      1   2374   0.003450  1709  0.251250\n",
      "home            home           4      1   2362   0.003433  1687  0.248015\n",
      "member        member           6      1   2213   0.003216  1477  0.217142\n",
      "\n",
      "Bottom 25 words:\n",
      "                                          Phrase  Characters  Terms  Count  \\\n",
      "waithera                                waithera           8      1     11   \n",
      "good business community  good business community          23      3     11   \n",
      "joined group following    joined group following          22      3     11   \n",
      "beneficial                            beneficial          10      1     11   \n",
      "cover cost                            cover cost          10      2     11   \n",
      "benefited lot training    benefited lot training          22      3     11   \n",
      "old woman mother                old woman mother          16      3     11   \n",
      "old woman married              old woman married          17      3     11   \n",
      "old woman living                old woman living          16      3     11   \n",
      "venus woman group              venus woman group          17      3     11   \n",
      "beans rice                            beans rice          10      2     11   \n",
      "bear expense family          bear expense family          19      3     11   \n",
      "beauty product woman        beauty product woman          20      3     11   \n",
      "venturing                              venturing           9      1     11   \n",
      "venus                                      venus           5      1     11   \n",
      "venus woman                          venus woman          11      2     11   \n",
      "old married wife                old married wife          16      3     11   \n",
      "old widow life                    old widow life          14      3     11   \n",
      "girl aged                              girl aged           9      2     11   \n",
      "beauty supply                      beauty supply          13      2     11   \n",
      "old mother 3                        old mother 3          12      3     11   \n",
      "skilled entrepreneur        skilled entrepreneur          20      2     11   \n",
      "villa del                              villa del           9      2     11   \n",
      "villa del carmen                villa del carmen          16      3     11   \n",
      "rongo district kenya        rongo district kenya          20      3     11   \n",
      "\n",
      "                         Count Pct  Docs  Docs Pct  \n",
      "waithera                  0.000016    11  0.001617  \n",
      "good business community   0.000016    11  0.001617  \n",
      "joined group following    0.000016    11  0.001617  \n",
      "beneficial                0.000016    11  0.001617  \n",
      "cover cost                0.000016    11  0.001617  \n",
      "benefited lot training    0.000016    11  0.001617  \n",
      "old woman mother          0.000016    11  0.001617  \n",
      "old woman married         0.000016    11  0.001617  \n",
      "old woman living          0.000016    11  0.001617  \n",
      "venus woman group         0.000016    10  0.001470  \n",
      "beans rice                0.000016    11  0.001617  \n",
      "bear expense family       0.000016    11  0.001617  \n",
      "beauty product woman      0.000016    11  0.001617  \n",
      "venturing                 0.000016    11  0.001617  \n",
      "venus                     0.000016    10  0.001470  \n",
      "venus woman               0.000016    10  0.001470  \n",
      "old married wife          0.000016    11  0.001617  \n",
      "old widow life            0.000016    11  0.001617  \n",
      "girl aged                 0.000016    11  0.001617  \n",
      "beauty supply             0.000016    11  0.001617  \n",
      "old mother 3              0.000016    11  0.001617  \n",
      "skilled entrepreneur      0.000016    11  0.001617  \n",
      "villa del                 0.000016    10  0.001470  \n",
      "villa del carmen          0.000016    10  0.001470  \n",
      "rongo district kenya      0.000016    11  0.001617  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"kiva_cleaned\", id_col=\"loan_id\", text_col=\"en_clean\", target_cols=['status','country', 'gender', 'nonpayment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3230\n",
      "Number of word forms (terms): 6692\n",
      "Number of words (tokens): 29180\n",
      "Mean words per document: 9.0\n",
      "Mean term occurance: 4.4\n",
      "Number (Pct) of terms occuring <= 1: 4076 (60.9)\n",
      "Number (Pct) of terms occuring <= 5: 5860 (87.6)\n",
      "Number (Pct) of terms occuring <= 10: 6223 (93.0)\n",
      "Number (Pct) of terms occuring <= 100: 6666 (99.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                          Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "president              president           9      1   1271   0.047095  1262   \n",
      "obama                      obama           5      1   1165   0.043167  1163   \n",
      "president obama  president obama          15      2   1136   0.042093  1136   \n",
      "#actonclimate      #actonclimate          13      1    307   0.011375   307   \n",
      "change                    change           6      1    236   0.008745   230   \n",
      "american                american           8      1    223   0.008263   222   \n",
      "watch                      watch           5      1    210   0.007781   210   \n",
      "climate                  climate           7      1    207   0.007670   199   \n",
      "@whitehouse          @whitehouse          11      1    201   0.007448   201   \n",
      "time                        time           4      1    190   0.007040   188   \n",
      "today                      today           5      1    179   0.006633   179   \n",
      "climate change    climate change          14      2    179   0.006633   177   \n",
      "health                    health           6      1    174   0.006447   172   \n",
      "make                        make           4      1    165   0.006114   163   \n",
      "senate                    senate           6      1    156   0.005780   156   \n",
      "america                  america           7      1    155   0.005743   154   \n",
      "year                        year           4      1    155   0.005743   147   \n",
      "@ofa                        @ofa           4      1    155   0.005743   155   \n",
      "economy                  economy           7      1    154   0.005706   154   \n",
      "#doyourjob            #doyourjob          10      1    150   0.005558   150   \n",
      "live                        live           4      1    148   0.005484   147   \n",
      "job                          job           3      1    145   0.005373   123   \n",
      "read                        read           4      1    136   0.005039   136   \n",
      "#sotu                      #sotu           5      1    125   0.004632   125   \n",
      "live president    live president          14      2    125   0.004632   125   \n",
      "\n",
      "                 Docs Pct  \n",
      "president        0.390712  \n",
      "obama            0.360062  \n",
      "president obama  0.351703  \n",
      "#actonclimate    0.095046  \n",
      "change           0.071207  \n",
      "american         0.068731  \n",
      "watch            0.065015  \n",
      "climate          0.061610  \n",
      "@whitehouse      0.062229  \n",
      "time             0.058204  \n",
      "today            0.055418  \n",
      "climate change   0.054799  \n",
      "health           0.053251  \n",
      "make             0.050464  \n",
      "senate           0.048297  \n",
      "america          0.047678  \n",
      "year             0.045511  \n",
      "@ofa             0.047988  \n",
      "economy          0.047678  \n",
      "#doyourjob       0.046440  \n",
      "live             0.045511  \n",
      "job              0.038080  \n",
      "read             0.042105  \n",
      "#sotu            0.038700  \n",
      "live president   0.038700  \n",
      "\n",
      "Bottom 25 words:\n",
      "                                              Phrase  Characters  Terms  \\\n",
      "court nomination                    court nomination          16      2   \n",
      "hearing timely                        hearing timely          14      2   \n",
      "continues                                  continues           9      1   \n",
      "hearing timely vote              hearing timely vote          19      3   \n",
      "consider                                    consider           8      1   \n",
      "consecutive month private  consecutive month private          25      3   \n",
      "help #stopgunviolence          help #stopgunviolence          21      2   \n",
      "highest                                      highest           7      1   \n",
      "honor                                          honor           5      1   \n",
      "garland deserves fair          garland deserves fair          21      3   \n",
      "shutdown                                    shutdown           8      1   \n",
      "class families                        class families          14      2   \n",
      "chief                                          chief           5      1   \n",
      "check president obama          check president obama          21      3   \n",
      "time congress                          time congress          13      2   \n",
      "immigration reform                immigration reform          18      2   \n",
      "taken                                          taken           5      1   \n",
      "july                                            july           4      1   \n",
      "democracy                                  democracy           9      1   \n",
      "sign petition                          sign petition          13      2   \n",
      "vote #doyourjob                      vote #doyourjob          15      2   \n",
      "sick leave                                sick leave          10      2   \n",
      "late                                            late           4      1   \n",
      "bold action                              bold action          11      2   \n",
      "gained                                        gained           6      1   \n",
      "\n",
      "                           Count  Count Pct  Docs  Docs Pct  \n",
      "court nomination              10   0.000371    10  0.003096  \n",
      "hearing timely                10   0.000371    10  0.003096  \n",
      "continues                     10   0.000371    10  0.003096  \n",
      "hearing timely vote           10   0.000371    10  0.003096  \n",
      "consider                      10   0.000371    10  0.003096  \n",
      "consecutive month private     10   0.000371    10  0.003096  \n",
      "help #stopgunviolence         10   0.000371    10  0.003096  \n",
      "highest                       10   0.000371    10  0.003096  \n",
      "honor                         10   0.000371    10  0.003096  \n",
      "garland deserves fair         10   0.000371    10  0.003096  \n",
      "shutdown                      10   0.000371    10  0.003096  \n",
      "class families                10   0.000371    10  0.003096  \n",
      "chief                         10   0.000371    10  0.003096  \n",
      "check president obama         10   0.000371    10  0.003096  \n",
      "time congress                 10   0.000371    10  0.003096  \n",
      "immigration reform            10   0.000371    10  0.003096  \n",
      "taken                         10   0.000371    10  0.003096  \n",
      "july                          10   0.000371    10  0.003096  \n",
      "democracy                     10   0.000371    10  0.003096  \n",
      "sign petition                 10   0.000371    10  0.003096  \n",
      "vote #doyourjob               10   0.000371    10  0.003096  \n",
      "sick leave                    10   0.000371    10  0.003096  \n",
      "late                          10   0.000371    10  0.003096  \n",
      "bold action                   10   0.000371    10  0.003096  \n",
      "gained                        10   0.000371    10  0.003096  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"obama_tweets\", id_col=\"id\", text_col=\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/3YSXZq1EVj\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/6Ve3YJoStm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/Kgkz1Z3FwJ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2819\n",
      "Number of word forms (terms): 9431\n",
      "Number of words (tokens): 25109\n",
      "Mean words per document: 8.9\n",
      "Mean term occurance: 2.7\n",
      "Number (Pct) of terms occuring <= 1: 6296 (66.8)\n",
      "Number (Pct) of terms occuring <= 5: 8669 (91.9)\n",
      "Number (Pct) of terms occuring <= 10: 9098 (96.5)\n",
      "Number (Pct) of terms occuring <= 100: 9420 (99.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                    Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "tesla                tesla           5      1    342   0.027297   335   \n",
      "model                model           5      1    244   0.019475   233   \n",
      "@spacex            @spacex           7      1    172   0.013728   171   \n",
      "just                  just           4      1    162   0.012930   161   \n",
      "@teslamotors  @teslamotors          12      1    159   0.012691   158   \n",
      "launch              launch           6      1    157   0.012531   145   \n",
      "car                    car           3      1    150   0.011972   139   \n",
      "rocket              rocket           6      1    142   0.011334   142   \n",
      "good                  good           4      1    140   0.011174   139   \n",
      "like                  like           4      1    120   0.009578   114   \n",
      "falcon              falcon           6      1    108   0.008620   106   \n",
      "yes                    yes           3      1    101   0.008061   101   \n",
      "1                        1           1      1     92   0.007343    84   \n",
      "dragon              dragon           6      1     89   0.007104    85   \n",
      "time                  time           4      1     88   0.007024    87   \n",
      "landing            landing           7      1     88   0.007024    80   \n",
      "@elonmusk        @elonmusk           9      1     87   0.006944    87   \n",
      "3                        3           1      1     86   0.006864    84   \n",
      "n                        n           1      1     85   0.006784    81   \n",
      "9                        9           1      1     83   0.006625    82   \n",
      "year                  year           4      1     80   0.006385    75   \n",
      "new                    new           3      1     80   0.006385    78   \n",
      "thanks              thanks           6      1     73   0.005826    73   \n",
      "great                great           5      1     72   0.005747    70   \n",
      "x                        x           1      1     70   0.005587    68   \n",
      "\n",
      "              Docs Pct  \n",
      "tesla         0.118836  \n",
      "model         0.082653  \n",
      "@spacex       0.060660  \n",
      "just          0.057112  \n",
      "@teslamotors  0.056048  \n",
      "launch        0.051437  \n",
      "car           0.049308  \n",
      "rocket        0.050372  \n",
      "good          0.049308  \n",
      "like          0.040440  \n",
      "falcon        0.037602  \n",
      "yes           0.035828  \n",
      "1             0.029798  \n",
      "dragon        0.030153  \n",
      "time          0.030862  \n",
      "landing       0.028379  \n",
      "@elonmusk     0.030862  \n",
      "3             0.029798  \n",
      "n             0.028734  \n",
      "9             0.029088  \n",
      "year          0.026605  \n",
      "new           0.027669  \n",
      "thanks        0.025896  \n",
      "great         0.024832  \n",
      "x             0.024122  \n",
      "\n",
      "Bottom 25 words:\n",
      "                      Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "comment              comment           7      1     10   0.000798    10   \n",
      "build                  build           5      1     10   0.000798    10   \n",
      "network              network           7      1     10   0.000798    10   \n",
      "clear                  clear           5      1     10   0.000798    10   \n",
      "medium                medium           6      1     10   0.000798    10   \n",
      "totally              totally           7      1     10   0.000798    10   \n",
      "travel                travel           6      1     10   0.000798    10   \n",
      "tested                tested           6      1     10   0.000798    10   \n",
      "single                single           6      1     10   0.000798    10   \n",
      "test flight      test flight          11      2     10   0.000798    10   \n",
      "built                  built           5      1     10   0.000798    10   \n",
      "static                static           6      1     10   0.000798    10   \n",
      "option                option           6      1     10   0.000798    10   \n",
      "@theeconomist  @theeconomist          13      1     10   0.000798    10   \n",
      "@                          @           1      1     10   0.000798    10   \n",
      "taking                taking           6      1     10   0.000798    10   \n",
      "tesla owner      tesla owner          11      2     10   0.000798    10   \n",
      "advance              advance           7      1     10   0.000798    10   \n",
      "auto dealer      auto dealer          11      2     10   0.000798    10   \n",
      "congrats            congrats           8      1     10   0.000798    10   \n",
      "oxygen                oxygen           6      1     10   0.000798    10   \n",
      "gen                      gen           3      1     10   0.000798    10   \n",
      "value                  value           5      1     10   0.000798    10   \n",
      "market                market           6      1     10   0.000798    10   \n",
      "fun                      fun           3      1     10   0.000798    10   \n",
      "\n",
      "               Docs Pct  \n",
      "comment        0.003547  \n",
      "build          0.003547  \n",
      "network        0.003547  \n",
      "clear          0.003547  \n",
      "medium         0.003547  \n",
      "totally        0.003547  \n",
      "travel         0.003547  \n",
      "tested         0.003547  \n",
      "single         0.003547  \n",
      "test flight    0.003547  \n",
      "built          0.003547  \n",
      "static         0.003547  \n",
      "option         0.003547  \n",
      "@theeconomist  0.003547  \n",
      "@              0.003547  \n",
      "taking         0.003547  \n",
      "tesla owner    0.003547  \n",
      "advance        0.003547  \n",
      "auto dealer    0.003547  \n",
      "congrats       0.003547  \n",
      "oxygen         0.003547  \n",
      "gen            0.003547  \n",
      "value          0.003547  \n",
      "market         0.003547  \n",
      "fun            0.003547  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"elonmusk_tweets\", id_col=\"id\", text_col=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/ZQ0osiFEJQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/SmTkLPiBYD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/T5JBFXOz3F\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"http://t.co/PtViAyrO4A\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30385\n",
      "Number of word forms (terms): 57771\n",
      "Number of words (tokens): 290035\n",
      "Mean words per document: 9.5\n",
      "Mean term occurance: 5.0\n",
      "Number (Pct) of terms occuring <= 1: 39447 (68.3)\n",
      "Number (Pct) of terms occuring <= 5: 51735 (89.6)\n",
      "Number (Pct) of terms occuring <= 10: 54232 (93.9)\n",
      "Number (Pct) of terms occuring <= 100: 57423 (99.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                            Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "@realdonaldtrump  @realdonaldtrump          16      1   8516   0.031999  8448   \n",
      "trump                        trump           5      1   4550   0.017097  4311   \n",
      "great                        great           5      1   3852   0.014474  3616   \n",
      "thanks                      thanks           6      1   2127   0.007992  2115   \n",
      "thank                        thank           5      1   1959   0.007361  1934   \n",
      "president                president           9      1   1846   0.006936  1796   \n",
      "donald                      donald           6      1   1698   0.006380  1669   \n",
      "just                          just           4      1   1669   0.006271  1627   \n",
      "obama                        obama           5      1   1458   0.005479  1409   \n",
      "people                      people           6      1   1339   0.005031  1286   \n",
      "like                          like           4      1   1241   0.004663  1204   \n",
      "america                    america           7      1   1236   0.004644  1192   \n",
      "make                          make           4      1   1230   0.004622  1190   \n",
      "new                            new           3      1   1201   0.004513  1164   \n",
      "need                          need           4      1   1160   0.004359  1104   \n",
      "time                          time           4      1   1139   0.004280  1097   \n",
      "donald trump          donald trump          12      2   1129   0.004242  1120   \n",
      "run                            run           3      1   1124   0.004223  1050   \n",
      "country                    country           7      1   1078   0.004051  1049   \n",
      "good                          good           4      1   1062   0.003991  1035   \n",
      "love                          love           4      1    947   0.003558   907   \n",
      "#trump2016              #trump2016          10      1    913   0.003431   912   \n",
      "think                        think           5      1    885   0.003325   841   \n",
      "best                          best           4      1    860   0.003231   831   \n",
      "big                            big           3      1    820   0.003081   761   \n",
      "\n",
      "                  Docs Pct  \n",
      "@realdonaldtrump  0.278032  \n",
      "trump             0.141879  \n",
      "great             0.119006  \n",
      "thanks            0.069607  \n",
      "thank             0.063650  \n",
      "president         0.059108  \n",
      "donald            0.054928  \n",
      "just              0.053546  \n",
      "obama             0.046372  \n",
      "people            0.042324  \n",
      "like              0.039625  \n",
      "america           0.039230  \n",
      "make              0.039164  \n",
      "new               0.038308  \n",
      "need              0.036334  \n",
      "time              0.036103  \n",
      "donald trump      0.036860  \n",
      "run               0.034557  \n",
      "country           0.034524  \n",
      "good              0.034063  \n",
      "love              0.029850  \n",
      "#trump2016        0.030015  \n",
      "think             0.027678  \n",
      "best              0.027349  \n",
      "big               0.025045  \n",
      "\n",
      "Bottom 25 words:\n",
      "                            Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "electoral                electoral           9      1     10   0.000038    10   \n",
      "economically          economically          12      1     10   0.000038    10   \n",
      "economic growth    economic growth          15      2     10   0.000038    10   \n",
      "dumb rock                dumb rock           9      2     10   0.000038    10   \n",
      "drew                          drew           4      1     10   0.000038    10   \n",
      "doubled                    doubled           7      1     10   0.000038    10   \n",
      "entrepreneurship  entrepreneurship          16      1     10   0.000038    10   \n",
      "environmental        environmental          13      1     10   0.000038    10   \n",
      "equal                        equal           5      1     10   0.000038    10   \n",
      "think right            think right          11      2     10   0.000038    10   \n",
      "funded                      funded           6      1     10   0.000038    10   \n",
      "forcing                    forcing           7      1     10   0.000038    10   \n",
      "fold                          fold           4      1     10   0.000038    10   \n",
      "fifth avenue          fifth avenue          12      2     10   0.000038    10   \n",
      "think big donald  think big donald          16      3     10   0.000038    10   \n",
      "father fred            father fred          11      2     10   0.000038    10   \n",
      "thinking small      thinking small          14      2     10   0.000038    10   \n",
      "este                          este           4      1     10   0.000038    10   \n",
      "fan love                  fan love           8      2     10   0.000038    10   \n",
      "fake news                fake news           9      2     10   0.000038    10   \n",
      "fairly                      fairly           6      1     10   0.000038    10   \n",
      "failure fatal        failure fatal          13      2     10   0.000038    10   \n",
      "exist                        exist           5      1     10   0.000038    10   \n",
      "exchange                  exchange           8      1     10   0.000038    10   \n",
      "incorrect                incorrect           9      1     10   0.000038    10   \n",
      "\n",
      "                  Docs Pct  \n",
      "electoral         0.000329  \n",
      "economically      0.000329  \n",
      "economic growth   0.000329  \n",
      "dumb rock         0.000329  \n",
      "drew              0.000329  \n",
      "doubled           0.000329  \n",
      "entrepreneurship  0.000329  \n",
      "environmental     0.000329  \n",
      "equal             0.000329  \n",
      "think right       0.000329  \n",
      "funded            0.000329  \n",
      "forcing           0.000329  \n",
      "fold              0.000329  \n",
      "fifth avenue      0.000329  \n",
      "think big donald  0.000329  \n",
      "father fred       0.000329  \n",
      "thinking small    0.000329  \n",
      "este              0.000329  \n",
      "fan love          0.000329  \n",
      "fake news         0.000329  \n",
      "fairly            0.000329  \n",
      "failure fatal     0.000329  \n",
      "exist             0.000329  \n",
      "exchange          0.000329  \n",
      "incorrect         0.000329  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"2017_trump_tweets\", id_col=\"id\", text_col=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3339\n",
      "Number of word forms (terms): 9497\n",
      "Number of words (tokens): 37374\n",
      "Mean words per document: 11.2\n",
      "Mean term occurance: 3.9\n",
      "Number (Pct) of terms occuring <= 1: 5574 (58.7)\n",
      "Number (Pct) of terms occuring <= 5: 8291 (87.3)\n",
      "Number (Pct) of terms occuring <= 10: 8855 (93.2)\n",
      "Number (Pct) of terms occuring <= 100: 9478 (99.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                    Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "@indiana          @indiana           8      1    519   0.019511   513   \n",
      "indiana            indiana           7      1    519   0.019511   506   \n",
      "edc                    edc           3      1    508   0.019098   503   \n",
      "@indiana edc  @indiana edc          12      2    508   0.019098   503   \n",
      "hoosier            hoosier           7      1    453   0.017030   444   \n",
      "state                state           5      1    329   0.012368   314   \n",
      "today                today           5      1    325   0.012218   324   \n",
      "@firstladyin  @firstladyin          12      1    320   0.012030   320   \n",
      "@govpencein    @govpencein          11      1    276   0.010376   276   \n",
      "new                    new           3      1    274   0.010301   252   \n",
      "job                    job           3      1    264   0.009925   257   \n",
      "w                        w           1      1    262   0.009850   253   \n",
      "#indiana          #indiana           8      1    234   0.008797   234   \n",
      "great                great           5      1    214   0.008045   207   \n",
      "year                  year           4      1    168   0.006316   161   \n",
      "2                        2           1      1    163   0.006128   152   \n",
      "business          business           8      1    141   0.005301   134   \n",
      "day                    day           3      1    128   0.004812   121   \n",
      "governor          governor           8      1    119   0.004474   116   \n",
      "1                        1           1      1    116   0.004361   113   \n",
      "@indot              @indot           6      1    110   0.004135   107   \n",
      "plan                  plan           4      1    110   0.004135   107   \n",
      "4                        4           1      1    109   0.004098    95   \n",
      "@indiana2016  @indiana2016          12      1    109   0.004098   109   \n",
      "family              family           6      1    106   0.003985   104   \n",
      "\n",
      "              Docs Pct  \n",
      "@indiana      0.153639  \n",
      "indiana       0.151542  \n",
      "edc           0.150644  \n",
      "@indiana edc  0.150644  \n",
      "hoosier       0.132974  \n",
      "state         0.094040  \n",
      "today         0.097035  \n",
      "@firstladyin  0.095837  \n",
      "@govpencein   0.082659  \n",
      "new           0.075472  \n",
      "job           0.076969  \n",
      "w             0.075771  \n",
      "#indiana      0.070081  \n",
      "great         0.061995  \n",
      "year          0.048218  \n",
      "2             0.045523  \n",
      "business      0.040132  \n",
      "day           0.036238  \n",
      "governor      0.034741  \n",
      "1             0.033842  \n",
      "@indot        0.032046  \n",
      "plan          0.032046  \n",
      "4             0.028452  \n",
      "@indiana2016  0.032645  \n",
      "family        0.031147  \n",
      "\n",
      "Bottom 25 words:\n",
      "                                        Phrase  Characters  Terms  Count  \\\n",
      "200 year                              200 year           8      2     10   \n",
      "business climate              business climate          16      2     10   \n",
      "head                                      head           4      1     10   \n",
      "heavy                                    heavy           5      1     10   \n",
      "trafficking                        trafficking          11      1     10   \n",
      "traffic                                traffic           7      1     10   \n",
      "bringing                              bringing           8      1     10   \n",
      "torch                                    torch           5      1     10   \n",
      "hiring                                  hiring           6      1     10   \n",
      "@subaru                                @subaru           7      1     10   \n",
      "4 5                                        4 5           3      2     10   \n",
      "today announced                today announced          15      2     10   \n",
      "add                                        add           3      1     10   \n",
      "hoosier business              hoosier business          16      2     10   \n",
      "@subaru usa                        @subaru usa          11      2     10   \n",
      "thousand                              thousand           8      1     10   \n",
      "hope                                      hope           4      1     10   \n",
      "hosting                                hosting           7      1     10   \n",
      "25                                          25           2      1     10   \n",
      "21st century crossroad  21st century crossroad          22      3     10   \n",
      "integrity                            integrity           9      1     10   \n",
      "jan                                        jan           3      1     10   \n",
      "january                                january           7      1     10   \n",
      "@firstladyin w                  @firstladyin w          14      2     10   \n",
      "@indiana edc board          @indiana edc board          18      3     10   \n",
      "\n",
      "                        Count Pct  Docs  Docs Pct  \n",
      "200 year                 0.000376    10  0.002995  \n",
      "business climate         0.000376    10  0.002995  \n",
      "head                     0.000376    10  0.002995  \n",
      "heavy                    0.000376    10  0.002995  \n",
      "trafficking              0.000376    10  0.002995  \n",
      "traffic                  0.000376    10  0.002995  \n",
      "bringing                 0.000376    10  0.002995  \n",
      "torch                    0.000376    10  0.002995  \n",
      "hiring                   0.000376    10  0.002995  \n",
      "@subaru                  0.000376    10  0.002995  \n",
      "4 5                      0.000376    10  0.002995  \n",
      "today announced          0.000376    10  0.002995  \n",
      "add                      0.000376    10  0.002995  \n",
      "hoosier business         0.000376    10  0.002995  \n",
      "@subaru usa              0.000376    10  0.002995  \n",
      "thousand                 0.000376    10  0.002995  \n",
      "hope                     0.000376    10  0.002995  \n",
      "hosting                  0.000376    10  0.002995  \n",
      "25                       0.000376    10  0.002995  \n",
      "21st century crossroad   0.000376    10  0.002995  \n",
      "integrity                0.000376    10  0.002995  \n",
      "jan                      0.000376    10  0.002995  \n",
      "january                  0.000376    10  0.002995  \n",
      "@firstladyin w           0.000376    10  0.002995  \n",
      "@indiana edc board       0.000376    10  0.002995  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"pence_tweets\", id_col=\"id\", text_col=\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 19779\n",
      "Number of word forms (terms): 106064\n",
      "Number of words (tokens): 1631684\n",
      "Mean words per document: 82.5\n",
      "Mean term occurance: 15.4\n",
      "Number (Pct) of terms occuring <= 1: 57107 (53.8)\n",
      "Number (Pct) of terms occuring <= 5: 86891 (81.9)\n",
      "Number (Pct) of terms occuring <= 10: 93905 (88.5)\n",
      "Number (Pct) of terms occuring <= 100: 103810 (97.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "            Phrase  Characters  Terms  Count  Count Pct   Docs  Docs Pct\n",
      "said          said           4      1  53019   0.026081  15375  0.777340\n",
      "mln            mln           3      1  25716   0.012650   8341  0.421710\n",
      "dlrs          dlrs           4      1  20760   0.010212   7730  0.390819\n",
      "reuter      reuter           6      1  19139   0.009415  19076  0.964457\n",
      "pct            pct           3      1  17213   0.008468   6010  0.303858\n",
      "1                1           1      1  16849   0.008288   7170  0.362506\n",
      "v                v           1      1  14830   0.007295   3192  0.161383\n",
      "year          year           4      1  13658   0.006719   6450  0.326103\n",
      "000            000           3      1  13427   0.006605   4761  0.240710\n",
      "company    company           7      1  10984   0.005403   6022  0.304464\n",
      "2                2           1      1  10415   0.005123   5483  0.277213\n",
      "billion    billion           7      1  10317   0.005075   3264  0.165024\n",
      "u                u           1      1   9597   0.004721   4177  0.211184\n",
      "bank          bank           4      1   9530   0.004688   3077  0.155569\n",
      "ct              ct           2      1   9048   0.004451   3434  0.173618\n",
      "share        share           5      1   8545   0.004204   3795  0.191870\n",
      "3                3           1      1   8379   0.004122   4729  0.239092\n",
      "mln dlrs  mln dlrs           8      2   7980   0.003926   3854  0.194853\n",
      "5                5           1      1   7411   0.003646   4444  0.224683\n",
      "4                4           1      1   7256   0.003569   4050  0.204763\n",
      "market      market           6      1   6679   0.003286   3327  0.168209\n",
      "new            new           3      1   6527   0.003211   3823  0.193286\n",
      "net            net           3      1   6107   0.003004   3267  0.165175\n",
      "corp          corp           4      1   6044   0.002973   4457  0.225340\n",
      "stock        stock           5      1   5913   0.002909   3039  0.153648\n",
      "\n",
      "Bottom 25 words:\n",
      "                                    Phrase  Characters  Terms  Count  \\\n",
      "macao                                macao           5      1     33   \n",
      "warrant exercisable    warrant exercisable          19      2     33   \n",
      "apollo                              apollo           6      1     33   \n",
      "energy said                    energy said          11      2     33   \n",
      "denomination mln yen  denomination mln yen          20      3     33   \n",
      "past month                      past month          10      2     33   \n",
      "mainframe                        mainframe           9      1     33   \n",
      "industrial output        industrial output          17      2     33   \n",
      "494                                    494           3      1     33   \n",
      "week u                              week u           6      2     33   \n",
      "half hour                        half hour           9      2     33   \n",
      "automatically                automatically          13      1     33   \n",
      "weizsaecker                    weizsaecker          11      1     33   \n",
      "august 31                        august 31           9      2     33   \n",
      "union pacific                union pacific          13      2     33   \n",
      "attorney general          attorney general          16      2     33   \n",
      "1 85                                  1 85           4      2     33   \n",
      "municipal                        municipal           9      1     33   \n",
      "pct gdp                            pct gdp           7      2     33   \n",
      "lenders                            lenders           7      1     33   \n",
      "specified                        specified           9      1     33   \n",
      "462                                    462           3      1     33   \n",
      "spare                                spare           5      1     33   \n",
      "488                                    488           3      1     33   \n",
      "dlrs ton                          dlrs ton           8      2     33   \n",
      "\n",
      "                      Count Pct  Docs  Docs Pct  \n",
      "macao                  0.000016    12  0.000607  \n",
      "warrant exercisable    0.000016    29  0.001466  \n",
      "apollo                 0.000016    12  0.000607  \n",
      "energy said            0.000016    29  0.001466  \n",
      "denomination mln yen   0.000016    33  0.001668  \n",
      "past month             0.000016    33  0.001668  \n",
      "mainframe              0.000016    23  0.001163  \n",
      "industrial output      0.000016    27  0.001365  \n",
      "494                    0.000016    31  0.001567  \n",
      "week u                 0.000016    31  0.001567  \n",
      "half hour              0.000016    26  0.001315  \n",
      "automatically          0.000016    30  0.001517  \n",
      "weizsaecker            0.000016    13  0.000657  \n",
      "august 31              0.000016    32  0.001618  \n",
      "union pacific          0.000016    15  0.000758  \n",
      "attorney general       0.000016    24  0.001213  \n",
      "1 85                   0.000016    32  0.001618  \n",
      "municipal              0.000016    28  0.001416  \n",
      "pct gdp                0.000016    27  0.001365  \n",
      "lenders                0.000016    29  0.001466  \n",
      "specified              0.000016    32  0.001618  \n",
      "462                    0.000016    31  0.001567  \n",
      "spare                  0.000016    30  0.001517  \n",
      "488                    0.000016    33  0.001668  \n",
      "dlrs ton               0.000016    23  0.001163  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"reutersCSV\", id_col=\"pid\", text_col=\"doc.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4694\n",
      "Number of word forms (terms): 12584\n",
      "Number of words (tokens): 72235\n",
      "Mean words per document: 15.4\n",
      "Mean term occurance: 5.7\n",
      "Number (Pct) of terms occuring <= 1: 6967 (55.4)\n",
      "Number (Pct) of terms occuring <= 5: 10549 (83.8)\n",
      "Number (Pct) of terms occuring <= 10: 11400 (90.6)\n",
      "Number (Pct) of terms occuring <= 100: 12497 (99.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                    Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "president        president           9      1   1093   0.017811   986   \n",
      "trump                trump           5      1    981   0.015986   916   \n",
      "biden                biden           5      1    564   0.009191   550   \n",
      "american          american           8      1    560   0.009126   520   \n",
      "donald              donald           6      1    555   0.009044   549   \n",
      "donald trump  donald trump          12      2    546   0.008898   540   \n",
      "need                  need           4      1    526   0.008572   468   \n",
      "country            country           7      1    462   0.007529   450   \n",
      "vp                      vp           2      1    451   0.007349   450   \n",
      "today                today           5      1    426   0.006942   416   \n",
      "day                    day           3      1    418   0.006812   384   \n",
      "nation              nation           6      1    409   0.006665   394   \n",
      "people              people           6      1    403   0.006567   377   \n",
      "make                  make           4      1    403   0.006567   378   \n",
      "time                  time           4      1    382   0.006225   361   \n",
      "america            america           7      1    374   0.006095   365   \n",
      "health              health           6      1    338   0.005508   283   \n",
      "care                  care           4      1    313   0.005101   253   \n",
      "year                  year           4      1    311   0.005068   292   \n",
      "work                  work           4      1    293   0.004775   270   \n",
      "vp biden          vp biden           8      2    290   0.004726   290   \n",
      "just                  just           4      1    279   0.004547   269   \n",
      "right                right           5      1    249   0.004058   208   \n",
      "know                  know           4      1    239   0.003895   221   \n",
      "middle              middle           6      1    233   0.003797   223   \n",
      "\n",
      "              Docs Pct  \n",
      "president     0.210055  \n",
      "trump         0.195143  \n",
      "biden         0.117171  \n",
      "american      0.110780  \n",
      "donald        0.116958  \n",
      "donald trump  0.115040  \n",
      "need          0.099702  \n",
      "country       0.095867  \n",
      "vp            0.095867  \n",
      "today         0.088624  \n",
      "day           0.081807  \n",
      "nation        0.083937  \n",
      "people        0.080315  \n",
      "make          0.080528  \n",
      "time          0.076907  \n",
      "america       0.077759  \n",
      "health        0.060290  \n",
      "care          0.053899  \n",
      "year          0.062207  \n",
      "work          0.057520  \n",
      "vp biden      0.061781  \n",
      "just          0.057307  \n",
      "right         0.044312  \n",
      "know          0.047081  \n",
      "middle        0.047507  \n",
      "\n",
      "Bottom 25 words:\n",
      "                                      Phrase  Characters  Terms  Count  \\\n",
      "corner country                corner country          14      2     10   \n",
      "vital                                  vital           5      1     10   \n",
      "contract                            contract           8      1     10   \n",
      "parents income                parents income          14      2     10   \n",
      "spoke                                  spoke           5      1     10   \n",
      "lack                                    lack           4      1     10   \n",
      "staff                                  staff           5      1     10   \n",
      "nuestra                              nuestra           7      1     10   \n",
      "known                                  known           5      1     10   \n",
      "crisis head                      crisis head          11      2     10   \n",
      "donald trump year          donald trump year          17      3     10   \n",
      "crisis american              crisis american          15      2     10   \n",
      "officially                        officially          10      1     10   \n",
      "online                                online           6      1     10   \n",
      "advance                              advance           7      1     10   \n",
      "betsy                                  betsy           5      1     10   \n",
      "op                                        op           2      1     10   \n",
      "stand @nra                        stand @nra          10      2     10   \n",
      "joke                                    joke           4      1     10   \n",
      "join campaign                  join campaign          13      2     10   \n",
      "joe photo day                  joe photo day          13      3     10   \n",
      "stage tonight                  stage tonight          13      2     10   \n",
      "biden president obama  biden president obama          21      3     10   \n",
      "university                        university          10      1     10   \n",
      "hurt                                    hurt           4      1     10   \n",
      "\n",
      "                       Count Pct  Docs  Docs Pct  \n",
      "corner country          0.000163    10   0.00213  \n",
      "vital                   0.000163    10   0.00213  \n",
      "contract                0.000163    10   0.00213  \n",
      "parents income          0.000163    10   0.00213  \n",
      "spoke                   0.000163    10   0.00213  \n",
      "lack                    0.000163    10   0.00213  \n",
      "staff                   0.000163    10   0.00213  \n",
      "nuestra                 0.000163    10   0.00213  \n",
      "known                   0.000163    10   0.00213  \n",
      "crisis head             0.000163    10   0.00213  \n",
      "donald trump year       0.000163    10   0.00213  \n",
      "crisis american         0.000163    10   0.00213  \n",
      "officially              0.000163    10   0.00213  \n",
      "online                  0.000163    10   0.00213  \n",
      "advance                 0.000163    10   0.00213  \n",
      "betsy                   0.000163    10   0.00213  \n",
      "op                      0.000163    10   0.00213  \n",
      "stand @nra              0.000163    10   0.00213  \n",
      "joke                    0.000163    10   0.00213  \n",
      "join campaign           0.000163    10   0.00213  \n",
      "joe photo day           0.000163    10   0.00213  \n",
      "stage tonight           0.000163    10   0.00213  \n",
      "biden president obama   0.000163    10   0.00213  \n",
      "university              0.000163    10   0.00213  \n",
      "hurt                    0.000163    10   0.00213  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"JoeBidenTweets\", id_col=\"id\", text_col=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4999\n",
      "Number of word forms (terms): 32278\n",
      "Number of words (tokens): 219633\n",
      "Mean words per document: 43.9\n",
      "Mean term occurance: 6.8\n",
      "Number (Pct) of terms occuring <= 1: 20622 (63.9)\n",
      "Number (Pct) of terms occuring <= 5: 27813 (86.2)\n",
      "Number (Pct) of terms occuring <= 10: 29511 (91.4)\n",
      "Number (Pct) of terms occuring <= 100: 31928 (98.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "              Phrase  Characters  Terms  Count  Count Pct  Docs  Docs Pct\n",
      "like            like           4      1   3146   0.015443  1955  0.391078\n",
      "taste          taste           5      1   2819   0.013838  1889  0.377876\n",
      "good            good           4      1   2453   0.012041  1785  0.357071\n",
      "flavor        flavor           6      1   2378   0.011673  1556  0.311262\n",
      "coffee        coffee           6      1   1987   0.009754   737  0.147429\n",
      "just            just           4      1   1854   0.009101  1383  0.276655\n",
      "tea              tea           3      1   1585   0.007780   508  0.101620\n",
      "product      product           7      1   1492   0.007324  1018  0.203641\n",
      "great          great           5      1   1463   0.007181  1198  0.239648\n",
      "really        really           6      1   1301   0.006386   985  0.197039\n",
      "make            make           4      1   1277   0.006268  1004  0.200840\n",
      "love            love           4      1   1190   0.005841   945  0.189038\n",
      "little        little           6      1   1011   0.004963   791  0.158232\n",
      "use              use           3      1    995   0.004884   761  0.152230\n",
      "chocolate  chocolate           9      1    973   0.004776   466  0.093219\n",
      "cup              cup           3      1    948   0.004653   525  0.105021\n",
      "sugar          sugar           5      1    943   0.004629   587  0.117423\n",
      "did              did           3      1    909   0.004462   677  0.135427\n",
      "drink          drink           5      1    878   0.004310   507  0.101420\n",
      "sweet          sweet           5      1    863   0.004236   633  0.126625\n",
      "water          water           5      1    849   0.004168   485  0.097019\n",
      "time            time           4      1    777   0.003814   650  0.130026\n",
      "better        better           6      1    768   0.003770   657  0.131426\n",
      "bag              bag           3      1    745   0.003657   465  0.093019\n",
      "try              try           3      1    717   0.003520   626  0.125225\n",
      "\n",
      "Bottom 25 words:\n",
      "                            Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "instructions          instructions          12      1     10   0.000049    10   \n",
      "cranberries            cranberries          11      1     10   0.000049    10   \n",
      "little hard            little hard          11      2     10   0.000049    10   \n",
      "flavor add              flavor add          10      2     10   0.000049    10   \n",
      "little chocolate  little chocolate          16      2     10   0.000049    10   \n",
      "gary peterson        gary peterson          13      2     10   0.000049    10   \n",
      "fixed                        fixed           5      1     10   0.000049    10   \n",
      "crowd                        crowd           5      1     10   0.000049    10   \n",
      "crumble                    crumble           7      1     10   0.000049    10   \n",
      "airy                          airy           4      1     10   0.000049    10   \n",
      "crunchier                crunchier           9      1     10   0.000049    10   \n",
      "price reasonable  price reasonable          16      2     10   0.000049    10   \n",
      "price point            price point          11      2     10   0.000049    10   \n",
      "cup day                    cup day           7      2     10   0.000049    10   \n",
      "bit stronger          bit stronger          12      2     10   0.000049    10   \n",
      "gary                          gary           4      1     10   0.000049    10   \n",
      "cupboard                  cupboard           8      1     10   0.000049    10   \n",
      "steeping                  steeping           8      1     10   0.000049    10   \n",
      "childhood                childhood           9      1     10   0.000049    10   \n",
      "measured                  measured           8      1     10   0.000049    10   \n",
      "resist                      resist           6      1     10   0.000049    10   \n",
      "chia seeds              chia seeds          10      2     10   0.000049    10   \n",
      "free diet                free diet           9      2     10   0.000049    10   \n",
      "started eating      started eating          14      2     10   0.000049    10   \n",
      "smooth flavor        smooth flavor          13      2     10   0.000049    10   \n",
      "\n",
      "                  Docs Pct  \n",
      "instructions         0.002  \n",
      "cranberries          0.002  \n",
      "little hard          0.002  \n",
      "flavor add           0.002  \n",
      "little chocolate     0.002  \n",
      "gary peterson        0.002  \n",
      "fixed                0.002  \n",
      "crowd                0.002  \n",
      "crumble              0.002  \n",
      "airy                 0.002  \n",
      "crunchier            0.002  \n",
      "price reasonable     0.002  \n",
      "price point          0.002  \n",
      "cup day              0.002  \n",
      "bit stronger         0.002  \n",
      "gary                 0.002  \n",
      "cupboard             0.002  \n",
      "steeping             0.002  \n",
      "childhood            0.002  \n",
      "measured             0.002  \n",
      "resist               0.002  \n",
      "chia seeds           0.002  \n",
      "free diet            0.002  \n",
      "started eating       0.002  \n",
      "smooth flavor        0.002  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"amazon_food_reviews_10\", id_col=\"reviewID\", text_col=\"reviewText\", target_cols=['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4637\n",
      "Number of word forms (terms): 14332\n",
      "Number of words (tokens): 91674\n",
      "Mean words per document: 19.8\n",
      "Mean term occurance: 6.4\n",
      "Number (Pct) of terms occuring <= 1: 8957 (62.5)\n",
      "Number (Pct) of terms occuring <= 5: 12367 (86.3)\n",
      "Number (Pct) of terms occuring <= 10: 13150 (91.8)\n",
      "Number (Pct) of terms occuring <= 100: 14201 (99.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                    Phrase  Characters  Terms  Count  Count Pct  Docs  \\\n",
      "temple              temple           6      1   4692   0.050000  2588   \n",
      "place                place           5      1   2751   0.029316  2032   \n",
      "krishna            krishna           7      1   1884   0.020077  1197   \n",
      "visit                visit           5      1   1725   0.018382  1434   \n",
      "hare                  hare           4      1   1350   0.014386   459   \n",
      "good                  good           4      1   1208   0.012873  1009   \n",
      "bangalore        bangalore           9      1   1024   0.010912   886   \n",
      "iskcon              iskcon           6      1    824   0.008781   670   \n",
      "nice                  nice           4      1    800   0.008525   710   \n",
      "beautiful        beautiful           9      1    779   0.008301   708   \n",
      "food                  food           4      1    768   0.008184   650   \n",
      "time                  time           4      1    709   0.007555   589   \n",
      "lord                  lord           4      1    622   0.006628   495   \n",
      "hare krishna  hare krishna          12      2    555   0.005914   418   \n",
      "like                  like           4      1    547   0.005829   465   \n",
      "experience      experience          10      1    488   0.005200   431   \n",
      "inside              inside           6      1    483   0.005147   407   \n",
      "clean                clean           5      1    482   0.005136   467   \n",
      "visited            visited           7      1    461   0.004913   419   \n",
      "peaceful          peaceful           8      1    457   0.004870   451   \n",
      "peace                peace           5      1    450   0.004795   422   \n",
      "rama                  rama           4      1    446   0.004753   224   \n",
      "maintained      maintained          10      1    446   0.004753   437   \n",
      "great                great           5      1    423   0.004508   370   \n",
      "main                  main           4      1    414   0.004412   321   \n",
      "\n",
      "              Docs Pct  \n",
      "temple        0.558119  \n",
      "place         0.438214  \n",
      "krishna       0.258141  \n",
      "visit         0.309252  \n",
      "hare          0.098986  \n",
      "good          0.217598  \n",
      "bangalore     0.191072  \n",
      "iskcon        0.144490  \n",
      "nice          0.153116  \n",
      "beautiful     0.152685  \n",
      "food          0.140177  \n",
      "time          0.127022  \n",
      "lord          0.106750  \n",
      "hare krishna  0.090144  \n",
      "like          0.100280  \n",
      "experience    0.092948  \n",
      "inside        0.087772  \n",
      "clean         0.100712  \n",
      "visited       0.090360  \n",
      "peaceful      0.097261  \n",
      "peace         0.091007  \n",
      "rama          0.048307  \n",
      "maintained    0.094242  \n",
      "great         0.079793  \n",
      "main          0.069226  \n",
      "\n",
      "Bottom 25 words:\n",
      "                                                      Phrase  Characters  \\\n",
      "seen temple                                      seen temple          11   \n",
      "clean calm                                        clean calm          10   \n",
      "place evening                                  place evening          13   \n",
      "august 2014                                      august 2014          11   \n",
      "biggest temple                                biggest temple          14   \n",
      "lost                                                    lost           4   \n",
      "temple shop                                      temple shop          11   \n",
      "drop                                                    drop           4   \n",
      "temple sri                                        temple sri          10   \n",
      "space available                              space available          15   \n",
      "founder                                              founder           7   \n",
      "society krishna consciousness  society krishna consciousness          29   \n",
      "term                                                    term           4   \n",
      "social                                                social           6   \n",
      "lord venkateshwara                        lord venkateshwara          18   \n",
      "place best                                        place best          10   \n",
      "awesome experience                        awesome experience          18   \n",
      "time day                                            time day           8   \n",
      "size                                                    size           4   \n",
      "frequently                                        frequently          10   \n",
      "chaitanya                                          chaitanya           9   \n",
      "place bangalore visit                  place bangalore visit          21   \n",
      "sit meditate                                    sit meditate          12   \n",
      "living                                                living           6   \n",
      "place eat                                          place eat           9   \n",
      "\n",
      "                               Terms  Count  Count Pct  Docs  Docs Pct  \n",
      "seen temple                        2     10   0.000107    10  0.002157  \n",
      "clean calm                         2     10   0.000107    10  0.002157  \n",
      "place evening                      2     10   0.000107    10  0.002157  \n",
      "august 2014                        2     10   0.000107    10  0.002157  \n",
      "biggest temple                     2     10   0.000107    10  0.002157  \n",
      "lost                               1     10   0.000107    10  0.002157  \n",
      "temple shop                        2     10   0.000107    10  0.002157  \n",
      "drop                               1     10   0.000107    10  0.002157  \n",
      "temple sri                         2     10   0.000107    10  0.002157  \n",
      "space available                    2     10   0.000107    10  0.002157  \n",
      "founder                            1     10   0.000107    10  0.002157  \n",
      "society krishna consciousness      3     10   0.000107    10  0.002157  \n",
      "term                               1     10   0.000107    10  0.002157  \n",
      "social                             1     10   0.000107    10  0.002157  \n",
      "lord venkateshwara                 2     10   0.000107    10  0.002157  \n",
      "place best                         2     10   0.000107    10  0.002157  \n",
      "awesome experience                 2     10   0.000107    10  0.002157  \n",
      "time day                           2     10   0.000107    10  0.002157  \n",
      "size                               1     10   0.000107    10  0.002157  \n",
      "frequently                         1     10   0.000107    10  0.002157  \n",
      "chaitanya                          1     10   0.000107    10  0.002157  \n",
      "place bangalore visit              3     10   0.000107    10  0.002157  \n",
      "sit meditate                       2     10   0.000107    10  0.002157  \n",
      "living                             1     10   0.000107    10  0.002157  \n",
      "place eat                          2     10   0.000107    10  0.002157  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"ISKON_IMB767-XLS-ENG\", id_col=\"ID\", text_col=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 49994\n",
      "Number of word forms (terms): 152091\n",
      "Number of words (tokens): 2257681\n",
      "Mean words per document: 45.2\n",
      "Mean term occurance: 14.8\n",
      "Number (Pct) of terms occuring <= 1: 101591 (66.8)\n",
      "Number (Pct) of terms occuring <= 5: 131962 (86.8)\n",
      "Number (Pct) of terms occuring <= 10: 138894 (91.3)\n",
      "Number (Pct) of terms occuring <= 100: 149196 (98.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "              Phrase  Characters  Terms  Count  Count Pct   Docs  Docs Pct\n",
      "like            like           4      1  32445   0.013332  20044  0.400928\n",
      "taste          taste           5      1  29344   0.012058  19212  0.384286\n",
      "good            good           4      1  24510   0.010072  17643  0.352902\n",
      "flavor        flavor           6      1  23837   0.009795  15379  0.307617\n",
      "just            just           4      1  18716   0.007691  13825  0.276533\n",
      "coffee        coffee           6      1  18671   0.007672   7024  0.140497\n",
      "tea              tea           3      1  15787   0.006487   4776  0.095531\n",
      "great          great           5      1  15199   0.006246  12132  0.242669\n",
      "product      product           7      1  14536   0.005973   9903  0.198084\n",
      "make            make           4      1  12771   0.005248   9976  0.199544\n",
      "love            love           4      1  12119   0.004980   9765  0.195323\n",
      "really        really           6      1  11950   0.004911   9279  0.185602\n",
      "use              use           3      1  10374   0.004263   7776  0.155539\n",
      "chocolate  chocolate           9      1   9929   0.004080   4440  0.088811\n",
      "little        little           6      1   9920   0.004076   7853  0.157079\n",
      "cup              cup           3      1   9312   0.003826   5322  0.106453\n",
      "sugar          sugar           5      1   9269   0.003809   5633  0.112674\n",
      "did              did           3      1   8733   0.003589   6665  0.133316\n",
      "water          water           5      1   8732   0.003588   4868  0.097372\n",
      "drink          drink           5      1   8469   0.003480   4974  0.099492\n",
      "time            time           4      1   8240   0.003386   6727  0.134556\n",
      "sweet          sweet           5      1   7980   0.003279   5945  0.118914\n",
      "better        better           6      1   7845   0.003224   6659  0.133196\n",
      "doe              doe           3      1   7564   0.003108   6232  0.124655\n",
      "bag              bag           3      1   7429   0.003053   4701  0.094031\n",
      "\n",
      "Bottom 25 words:\n",
      "                                  Phrase  Characters  Terms  Count  Count Pct  \\\n",
      "ingredient label        ingredient label          16      2     33   0.000014   \n",
      "buy taste                      buy taste           9      2     33   0.000014   \n",
      "struck                            struck           6      1     33   0.000014   \n",
      "twists                            twists           6      1     33   0.000014   \n",
      "try drink                      try drink           9      2     33   0.000014   \n",
      "product people            product people          14      2     33   0.000014   \n",
      "nutritional profile  nutritional profile          19      2     33   0.000014   \n",
      "excellence                    excellence          10      1     33   0.000014   \n",
      "heavy cream                  heavy cream          11      2     33   0.000014   \n",
      "nutiva organic            nutiva organic          14      2     33   0.000014   \n",
      "summer berry blend    summer berry blend          18      3     33   0.000014   \n",
      "tried good                    tried good          10      2     33   0.000014   \n",
      "sealed bag                    sealed bag          10      2     33   0.000014   \n",
      "amazon grocery            amazon grocery          14      2     33   0.000014   \n",
      "casual                            casual           6      1     33   0.000014   \n",
      "kuerig                            kuerig           6      1     33   0.000014   \n",
      "work time                      work time           9      2     33   0.000014   \n",
      "plain white                  plain white          11      2     33   0.000014   \n",
      "nut free                        nut free           8      2     33   0.000014   \n",
      "healthy organic          healthy organic          15      2     33   0.000014   \n",
      "carbonated juice        carbonated juice          16      2     33   0.000014   \n",
      "juice energy drink    juice energy drink          18      3     33   0.000014   \n",
      "taste water                  taste water          11      2     33   0.000014   \n",
      "healthier option        healthier option          16      2     33   0.000014   \n",
      "tempting                        tempting           8      1     33   0.000014   \n",
      "\n",
      "                     Docs  Docs Pct  \n",
      "ingredient label       31   0.00062  \n",
      "buy taste              32   0.00064  \n",
      "struck                 32   0.00064  \n",
      "twists                 29   0.00058  \n",
      "try drink              33   0.00066  \n",
      "product people         33   0.00066  \n",
      "nutritional profile    30   0.00060  \n",
      "excellence             31   0.00062  \n",
      "heavy cream            26   0.00052  \n",
      "nutiva organic         26   0.00052  \n",
      "summer berry blend     21   0.00042  \n",
      "tried good             33   0.00066  \n",
      "sealed bag             32   0.00064  \n",
      "amazon grocery         33   0.00066  \n",
      "casual                 33   0.00066  \n",
      "kuerig                 31   0.00062  \n",
      "work time              33   0.00066  \n",
      "plain white            31   0.00062  \n",
      "nut free               32   0.00064  \n",
      "healthy organic        33   0.00066  \n",
      "carbonated juice       31   0.00062  \n",
      "juice energy drink     32   0.00064  \n",
      "taste water            32   0.00064  \n",
      "healthier option       31   0.00062  \n",
      "tempting               33   0.00066  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"reviews_Grocery_and_Gourmet_Food_5_50000\", id_col=\"reviewID\", text_col=\"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (35,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 75897\n",
      "Number of word forms (terms): 139306\n",
      "Number of words (tokens): 2653604\n",
      "Mean words per document: 35.0\n",
      "Mean term occurance: 19.0\n",
      "Number (Pct) of terms occuring <= 1: 85157 (61.1)\n",
      "Number (Pct) of terms occuring <= 5: 119316 (85.7)\n",
      "Number (Pct) of terms occuring <= 10: 127157 (91.3)\n",
      "Number (Pct) of terms occuring <= 100: 136693 (98.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                  Phrase  Characters  Terms  Count  Count Pct   Docs  Docs Pct\n",
      "pt                    pt           2      1  39184   0.009299  16767  0.220918\n",
      "vaccine          vaccine           7      1  38370   0.009106  23539  0.310144\n",
      "patient          patient           7      1  37727   0.008954  14754  0.194395\n",
      "received        received           8      1  36182   0.008587  23416  0.308523\n",
      "arm                  arm           3      1  25812   0.006126  17464  0.230101\n",
      "information  information          11      1  24000   0.005696  12506  0.164776\n",
      "day                  day           3      1  23232   0.005514  17099  0.225292\n",
      "reported        reported           8      1  20709   0.004915  12869  0.169559\n",
      "left                left           4      1  20622   0.004894  15154  0.199665\n",
      "injection      injection           9      1  20318   0.004822  14333  0.188848\n",
      "2                      2           1      1  20087   0.004767  14837  0.195489\n",
      "site                site           4      1  20045   0.004757  14946  0.196925\n",
      "dose                dose           4      1  19418   0.004608  12501  0.164710\n",
      "fever              fever           5      1  19382   0.004600  15387  0.202735\n",
      "10                    10           2      1  18566   0.004406  11741  0.154696\n",
      "developed      developed           9      1  17643   0.004187  14617  0.192590\n",
      "rash                rash           4      1  16943   0.004021  11471  0.151139\n",
      "pain                pain           4      1  16876   0.004005  10747  0.141600\n",
      "1                      1           1      1  16462   0.003907  11906  0.156870\n",
      "3                      3           1      1  16379   0.003887  12081  0.159176\n",
      "swelling        swelling           8      1  16291   0.003866  13205  0.173986\n",
      "03                    03           2      1  14976   0.003554   7487  0.098647\n",
      "5                      5           1      1  14689   0.003486  10941  0.144156\n",
      "4                      4           1      1  14149   0.003358  10577  0.139360\n",
      "vaccination  vaccination          11      1  14015   0.003326  10477  0.138042\n",
      "\n",
      "Bottom 25 words:\n",
      "                                          Phrase  Characters  Terms  Count  \\\n",
      "achieved                                achieved           8      1     62   \n",
      "regarding prevenar            regarding prevenar          18      2     62   \n",
      "heavy metal                          heavy metal          11      2     62   \n",
      "airway disease                    airway disease          14      2     62   \n",
      "benadryl 12 5                      benadryl 12 5          13      3     62   \n",
      "document                                document           8      1     62   \n",
      "30 hour                                  30 hour           7      2     62   \n",
      "11 month                                11 month           8      2     62   \n",
      "pt received vaccination  pt received vaccination          23      3     62   \n",
      "exposure prophylaxis        exposure prophylaxis          20      2     62   \n",
      "o nausea                                o nausea           8      2     62   \n",
      "auto                                        auto           4      1     62   \n",
      "arm reddened                        arm reddened          12      2     62   \n",
      "solid                                      solid           5      1     62   \n",
      "cousin                                    cousin           6      1     62   \n",
      "19 month old                        19 month old          12      3     62   \n",
      "advised pt                            advised pt          10      2     62   \n",
      "concerns                                concerns           8      1     62   \n",
      "state symptom                      state symptom          13      2     62   \n",
      "identifier                            identifier          10      1     62   \n",
      "tylenol cold                        tylenol cold          12      2     62   \n",
      "site reaction resolved    site reaction resolved          22      3     62   \n",
      "08 2004                                  08 2004           7      2     62   \n",
      "02 2                                        02 2           4      2     62   \n",
      "rash 1                                    rash 1           6      2     62   \n",
      "\n",
      "                         Count Pct  Docs  Docs Pct  \n",
      "achieved                  0.000015    46  0.000606  \n",
      "regarding prevenar        0.000015    62  0.000817  \n",
      "heavy metal               0.000015    33  0.000435  \n",
      "airway disease            0.000015    57  0.000751  \n",
      "benadryl 12 5             0.000015    62  0.000817  \n",
      "document                  0.000015    57  0.000751  \n",
      "30 hour                   0.000015    59  0.000777  \n",
      "11 month                  0.000015    60  0.000791  \n",
      "pt received vaccination   0.000015    60  0.000791  \n",
      "exposure prophylaxis      0.000015    57  0.000751  \n",
      "o nausea                  0.000015    60  0.000791  \n",
      "auto                      0.000015    56  0.000738  \n",
      "arm reddened              0.000015    61  0.000804  \n",
      "solid                     0.000015    59  0.000777  \n",
      "cousin                    0.000015    54  0.000711  \n",
      "19 month old              0.000015    60  0.000791  \n",
      "advised pt                0.000015    58  0.000764  \n",
      "concerns                  0.000015    61  0.000804  \n",
      "state symptom             0.000015    62  0.000817  \n",
      "identifier                0.000015    62  0.000817  \n",
      "tylenol cold              0.000015    62  0.000817  \n",
      "site reaction resolved    0.000015    62  0.000817  \n",
      "08 2004                   0.000015    55  0.000725  \n",
      "02 2                      0.000015    60  0.000791  \n",
      "rash 1                    0.000015    62  0.000817  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"vaers2\", id_col=\"VAERS_ID\", text_col=\"SYMPTOM_TEXT\", target_cols=['DIED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"http://www.suntimes.com/2072121,desiree-rogers-quits-022610.article\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"http://www.brookings.edu/—/media/Files/rc/reports/2010/08_arab_opinion_poll_telhami/08_arab_opinion_pollielhami.pdf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 6742\n",
      "Number of word forms (terms): 52659\n",
      "Number of words (tokens): 334681\n",
      "Mean words per document: 49.6\n",
      "Mean term occurance: 6.4\n",
      "Number (Pct) of terms occuring <= 1: 30458 (57.8)\n",
      "Number (Pct) of terms occuring <= 5: 44698 (84.9)\n",
      "Number (Pct) of terms occuring <= 10: 47886 (90.9)\n",
      "Number (Pct) of terms occuring <= 100: 52154 (99.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words:\n",
      "                Phrase  Characters  Terms  Count  Count Pct  Docs  Docs Pct\n",
      "state            state           5      1   2272   0.007100   628  0.093147\n",
      "pm                  pm           2      1   2119   0.006622   713  0.105755\n",
      "secretary    secretary           9      1   1387   0.004334   483  0.071640\n",
      "1                    1           1      1   1318   0.004119   525  0.077870\n",
      "u                    u           1      1   1254   0.003919   456  0.067636\n",
      "obama            obama           5      1   1244   0.003887   208  0.030851\n",
      "said              said           4      1   1233   0.003853   336  0.049837\n",
      "time              time           4      1   1228   0.003837   590  0.087511\n",
      "president    president           9      1   1149   0.003590   347  0.051468\n",
      "office          office           6      1   1140   0.003562   335  0.049689\n",
      "new                new           3      1   1096   0.003425   420  0.062296\n",
      "w                    w           1      1    971   0.003034   474  0.070306\n",
      "house            house           5      1    970   0.003031   297  0.044052\n",
      "10                  10           2      1    915   0.002859   442  0.065559\n",
      "2                    2           1      1    914   0.002856   355  0.052655\n",
      "30                  30           2      1    900   0.002812   321  0.047612\n",
      "2010              2010           4      1    894   0.002794   623  0.092406\n",
      "department  department          10      1    887   0.002772   295  0.043756\n",
      "just              just           4      1    885   0.002765   603  0.089439\n",
      "american      american           8      1    873   0.002728   218  0.032335\n",
      "2009              2009           4      1    868   0.002712   607  0.090033\n",
      "h                    h           1      1    844   0.002637   752  0.111540\n",
      "meeting        meeting           7      1    836   0.002612   374  0.055473\n",
      "fyi                fyi           3      1    829   0.002591   822  0.121922\n",
      "people          people           6      1    820   0.002562   295  0.043756\n",
      "\n",
      "Bottom 25 words:\n",
      "                                              Phrase  Characters  Terms  \\\n",
      "time meet                                  time meet           9      2   \n",
      "doe work                                    doe work           8      2   \n",
      "senior adviser                        senior adviser          14      2   \n",
      "huma monday                              huma monday          11      2   \n",
      "answered                                    answered           8      1   \n",
      "huma tuesday                            huma tuesday          12      2   \n",
      "agrees                                        agrees           6      1   \n",
      "trail                                          trail           5      1   \n",
      "margaret                                    margaret           8      1   \n",
      "ecial assistant secretary  ecial assistant secretary          25      3   \n",
      "9 15 weekly                              9 15 weekly          11      3   \n",
      "email sent                                email sent          10      2   \n",
      "email need immediate            email need immediate          20      3   \n",
      "abedin huma tuesday              abedin huma tuesday          19      3   \n",
      "mechanism                                  mechanism           9      1   \n",
      "high profile                            high profile          12      2   \n",
      "ecial                                          ecial           5      1   \n",
      "affair department                  affair department          17      2   \n",
      "materials                                  materials           9      1   \n",
      "massachusetts avenue nw      massachusetts avenue nw          23      3   \n",
      "adjust                                        adjust           6      1   \n",
      "mashabane                                  mashabane           9      1   \n",
      "pitch                                          pitch           5      1   \n",
      "earliest                                    earliest           8      1   \n",
      "ecial assistant                      ecial assistant          15      2   \n",
      "\n",
      "                           Count  Count Pct  Docs  Docs Pct  \n",
      "time meet                     10   0.000031    10  0.001483  \n",
      "doe work                      10   0.000031    10  0.001483  \n",
      "senior adviser                10   0.000031    10  0.001483  \n",
      "huma monday                   10   0.000031    10  0.001483  \n",
      "answered                      10   0.000031    10  0.001483  \n",
      "huma tuesday                  10   0.000031    10  0.001483  \n",
      "agrees                        10   0.000031    10  0.001483  \n",
      "trail                         10   0.000031    10  0.001483  \n",
      "margaret                      10   0.000031    10  0.001483  \n",
      "ecial assistant secretary     10   0.000031    10  0.001483  \n",
      "9 15 weekly                   10   0.000031    10  0.001483  \n",
      "email sent                    10   0.000031    10  0.001483  \n",
      "email need immediate          10   0.000031    10  0.001483  \n",
      "abedin huma tuesday           10   0.000031    10  0.001483  \n",
      "mechanism                     10   0.000031    10  0.001483  \n",
      "high profile                  10   0.000031    10  0.001483  \n",
      "ecial                         10   0.000031    10  0.001483  \n",
      "affair department             10   0.000031    10  0.001483  \n",
      "materials                     10   0.000031    10  0.001483  \n",
      "massachusetts avenue nw       10   0.000031    10  0.001483  \n",
      "adjust                        10   0.000031    10  0.001483  \n",
      "mashabane                     10   0.000031    10  0.001483  \n",
      "pitch                         10   0.000031    10  0.001483  \n",
      "earliest                      10   0.000031    10  0.001483  \n",
      "ecial assistant               10   0.000031    10  0.001483  \n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"Hillary_Emails\", id_col=\"Id\", text_col=\"ExtractedBodyText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_sklearn_kernel",
   "language": "python",
   "name": "small_sklearn_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
