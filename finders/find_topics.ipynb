{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find EDA\n",
    "\n",
    "- Stephen W. Thomas\n",
    "\n",
    "This script finds all named entities in a given corpus and outputs the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = 75\n",
    "n_top_words = 20\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['s', 'rt', 'br'])\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"i'd\": \"i would\",\n",
    "  \"i'd've\": \"i would have\",\n",
    "  \"i'll\": \"i will\",\n",
    "  \"i'll've\": \"i will have\",\n",
    "  \"i'm\": \"i am\",\n",
    "  \"i've\": \"i have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "\n",
    "\n",
    "# Simple preprocessor.\n",
    "# Note that this function will be called on each document before stop words are \n",
    "# removed, before lowercases, and before tokenization. I.e., the raw documents go into this function.\n",
    "def preprocessor(doc):\n",
    "\n",
    "    doc = doc.replace(u'’', u\"'\")\n",
    "    doc = doc.replace(u'“', u'\"')\n",
    "    doc = doc.replace(u'”', u'\"')\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    doc = BeautifulSoup(doc, \"lxml\").get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    \n",
    "    # remove URLS like pic.twitter.com/SODA\n",
    "    doc = re.sub(r'\\b\\S*(\\.com|\\.edu|\\.net|\\.gov|\\.ca|\\.org)(/\\S*)?', '', doc)\n",
    "    \n",
    "    # Make strings like \"@ DrJoe\" become \"@DrJoe\"\n",
    "    doc = re.sub(r'(\\@)(\\s+)(.)', r'\\1\\3', doc)\n",
    "    \n",
    "    # Make strings like \"# DrJoe\" become \"#DrJoe\"\n",
    "    doc = re.sub(r'(#)(\\s+)(.)', r'\\1\\3', doc)\n",
    "    \n",
    "\n",
    "\n",
    "    #spacy_doc = nlp(doc)\n",
    "    #doc = \" \".join([token.lemma_ for token in spacy_doc])\n",
    "    \n",
    "    #Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    doc = expandContractions(doc)\n",
    "    \n",
    "    doc = ' '.join([w for w in doc.split() if w not in stop_words])\n",
    "    \n",
    "    doc = ' '.join([lemmer.lemmatize(w) for w in doc.split()])\n",
    "    return doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'month pandemic proper testing, ppe, clear nationwide guidance. instead, donald trump: - push dangerous, disproven drug - stand way cdc - refuse wear mask failing basic test leadership.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"We're months into this pandemic and still don't have proper testing, PPE, or clear nationwide guidance. Instead, Donald Trump: - Pushes dangerous, disproven drugs - Stands in the way of the CDC - Refuses to wear a mask He is failing even the most basic test of leadership.\"\n",
    "preprocessor(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "def get_top_words(H, W, feature_names):\n",
    "    output = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_words = [(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        \n",
    "        # Metrics\n",
    "        #print(W[:,topic_idx])\n",
    "        support = np.count_nonzero(np.round(W[:,topic_idx], 3))\n",
    "        #print(support)\n",
    "        weight = W[:,topic_idx].sum()\n",
    "        avg_nonzero = weight/support\n",
    "        \n",
    "        output.append([str(topic_idx)] + [support, weight, avg_nonzero] + top_words)\n",
    "        \n",
    "    colnames = [\"Topic ID\", \"Support\", \"Weight\", \"Avg\"] +  [\"Word \"+str(i) for i in range(0, n_top_words)]\n",
    "    return pd.DataFrame(output, columns=colnames).sort_values(by=['Support'], ascending=False)\n",
    "\n",
    "def print_top_docs(topic_idx, W_df, data):\n",
    "    print(topic_idx)\n",
    "    top_doc_indices = np.argsort( W_df.iloc[:,topic_idx] )[::-1]\n",
    "    for doc_index in top_doc_indices[0:5]:\n",
    "        print(data.iloc[doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def find_topics(data_samples, text_col = \"tweet_text\", include_text=True):   \n",
    "    output = []\n",
    "      \n",
    "    # We override the token_pattern in order to keep @signs and #hashtags\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.80, min_df=10,\n",
    "                                   token_pattern = '[a-zA-Z0-9@#]+',\n",
    "                                   ngram_range={1,3},\n",
    "                                   stop_words=stop_words,\n",
    "                                   preprocessor=preprocessor,\n",
    "                                   max_features=n_features)\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "    \n",
    "    nmf = NMF(n_components=n_components, random_state=1, init='nndsvda', solver='mu', alpha=.1, l1_ratio=.5)\n",
    "    \n",
    "    W = nmf.fit_transform(tfidf)\n",
    "    H = nmf.components_\n",
    "    \n",
    "    # Normalize the W matrix\n",
    "    #row_sums = W.sum(axis=1)\n",
    "    #W = W / row_sums[:, np.newaxis]\n",
    "    W = normalize(W, axis=1, norm='l1')\n",
    "    \n",
    "    top_words = get_top_words(H, W, tfidf_vectorizer.get_feature_names())\n",
    "    print(top_words.head())\n",
    "    \n",
    "    W_df = pd.DataFrame(W, columns=[\"topic {}\".format(i) for i in range(n_components)])\n",
    "    H_df = pd.DataFrame(H, columns=tfidf_vectorizer.get_feature_names())\n",
    "    \n",
    "    return W_df, H_df, top_words   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it(file_base, id_col=\"id\", text_col=\"tweet_text\"):\n",
    "    df = pd.read_csv('../data/'+file_base+'.csv')\n",
    "    \n",
    "    # Drop rows without any text\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    \n",
    "    W_df, H_df, top_words = find_topics(df[text_col])\n",
    "\n",
    "    #print(top_words)\n",
    "    #for i in range(n_components):\n",
    "    #    print_top_docs(i, W_df, df[text_col])\n",
    "\n",
    "    pd.concat([df, W_df], axis=1).to_csv('out/'+file_base+'_W.csv', float_format='%.4f', index=False)\n",
    "    H_df.to_csv('out/'+file_base+'_H.csv', float_format='%.4f', index=True)\n",
    "    top_words.to_csv('out/'+file_base+'_top_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topic ID  Support      Weight       Avg     Word 0          Word 1  \\\n",
      "3        3     1007  114.993659  0.114194  president  need president   \n",
      "2        2      903  145.507214  0.161138     donald    donald trump   \n",
      "7        7      867  116.262536  0.134098   american          people   \n",
      "8        8      801   89.852129  0.112175    climate          change   \n",
      "1        1      737  171.367251  0.232520         vp        vp biden   \n",
      "\n",
      "            Word 2          Word 3     Word 4   Word 5  ...  \\\n",
      "3               mr    mr president       lead     term  ...   \n",
      "2            trump          nation        doe   defeat  ...   \n",
      "7  american people  people deserve     crisis  deserve  ...   \n",
      "8   climate change          action     threat  address  ...   \n",
      "1            biden            ohio  yesterday    night  ...   \n",
      "\n",
      "               Word 10             Word 11         Word 12   Word 13  \\\n",
      "3    running president             running          crisis  american   \n",
      "2  defeat donald trump                lead  term president      term   \n",
      "7       responsibility               think           tough    failed   \n",
      "8            emergency  existential threat          planet    urgent   \n",
      "1             tomorrow                talk    @barackobama   florida   \n",
      "\n",
      "           Word 14          Word 15          Word 16          Word 17  \\\n",
      "3  president obama           office  biden president           moment   \n",
      "2            unfit      coronavirus      year donald           office   \n",
      "7            brave         overcome           chance              war   \n",
      "8           tackle  address climate           future   climate crisis   \n",
      "1            break            watch     biden romney  vp biden romney   \n",
      "\n",
      "             Word 18               Word 19  \n",
      "3             george  trump term president  \n",
      "2  year donald trump      day donald trump  \n",
      "7          political               working  \n",
      "8        rally world                 rally  \n",
      "1                 10               college  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"JoeBidenTweets\", id_col=\"id\", text_col=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg Word 0      Word 1     Word 2  \\\n",
      "0         0     9559  704.769497  0.073728   make      little        end   \n",
      "1         1     6285  398.904223  0.063469  movie      movies       make   \n",
      "2         2     5654  356.764726  0.063100   film       films  film seen   \n",
      "14       14     4985  212.009649  0.042530   like        look  look like   \n",
      "24       24     4133  199.159908  0.048188   just  movie just  film just   \n",
      "\n",
      "        Word 3       Word 4     Word 5  ...    Word 10     Word 11   Word 12  \\\n",
      "0     director         come      point  ...       long       right     world   \n",
      "1   movie like          saw  recommend  ...     acting  movie just  horrible   \n",
      "2    film like  film making     cinema  ...  recommend         saw  film doe   \n",
      "14  movie like   like movie  feel like  ...  just like        feel     sound   \n",
      "24       plain    just like      going  ...       crap    horrible      need   \n",
      "\n",
      "     Word 13 Word 14   Word 15 Word 16       Word 17  Word 18         Word 19  \n",
      "0   audience   place       day    shot      actually      far            away  \n",
      "1    watched  review    boring    sure  movie really    enjoy     watch movie  \n",
      "2        art    seen  director   maker        boring  feature  cinematography  \n",
      "14       act   liked   looking   cheap          kind     type            hair  \n",
      "24       bit  saying    stupid    went          come      try       seriously  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"imdb.small\", id_col=\"id\", text_col=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg   Word 0         Word 1  \\\n",
      "18       18     2010  143.104704  0.071196     like     taste like   \n",
      "0         0     1902  117.038650  0.061535  product  great product   \n",
      "13       13     1860  131.868792  0.070897    taste     taste like   \n",
      "8         8     1846  137.548255  0.074512     good     taste good   \n",
      "14       14     1690  129.017084  0.076341   flavor            try   \n",
      "\n",
      "          Word 2       Word 3        Word 4       Word 5  ...     Word 10  \\\n",
      "18   really like       people           lot        think  ...        know   \n",
      "0   good product          way         happy      company  ...        used   \n",
      "13    taste good  taste great           doe    doe taste  ...  good taste   \n",
      "8         pretty  pretty good   really good   good taste  ...       tasty   \n",
      "14          mild  good flavor  great flavor  flavor good  ...      strong   \n",
      "\n",
      "          Word 11      Word 12      Word 13     Word 14       Word 15  \\\n",
      "18            try     probably  flavor like        real         sweet   \n",
      "0            able          new      ordered      people  disappointed   \n",
      "13  taste texture   like taste      texture  taste just         smell   \n",
      "8           brand  flavor good        quite   make good          hand   \n",
      "14     definitely          lot  flavor like     variety          pack   \n",
      "\n",
      "       Word 16    Word 17      Word 18    Word 19  \n",
      "18        feel        doe  like coffee    husband  \n",
      "0    excellent      think        issue       item  \n",
      "13  aftertaste     bitter         real  sweetener  \n",
      "8    expensive       size          got       feel  \n",
      "14    pleasant  wonderful       smooth        fan  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"amazon_food_reviews_10\", id_col=\"reviewID\", text_col=\"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg         Word 0           Word 1  \\\n",
      "0         0     1319  207.450842  0.157279          obama  president obama   \n",
      "15       15      458   94.848911  0.207094  #actonclimate            clean   \n",
      "1         1      442   96.899283  0.219229         change          climate   \n",
      "17       17      384   70.210363  0.182839       american          million   \n",
      "8         8      361   78.917437  0.218608            job          private   \n",
      "\n",
      "              Word 2           Word 3                        Word 4  \\\n",
      "0          president           nation                        future   \n",
      "15            energy            power                          plan   \n",
      "1     climate change    #actonclimate  climate change #actonclimate   \n",
      "17  million american  american worker                        worker   \n",
      "8             sector   private sector                        growth   \n",
      "\n",
      "                  Word 5  ...    Word 10          Word 11  \\\n",
      "0                federal  ...  americans         national   \n",
      "15          clean energy  ...      solar           record   \n",
      "1   change #actonclimate  ...     effect           global   \n",
      "17       american people  ...   security  security health   \n",
      "8                  month  ...    economy           record   \n",
      "\n",
      "                  Word 12              Word 13                        Word 14  \\\n",
      "0                stronger        #americaleads            obama #americaleads   \n",
      "15    fight #actonclimate  obama #actonclimate  president obama #actonclimate   \n",
      "1   climate change denier        change denier                         denier   \n",
      "17                 thanks    majority american                  auto industry   \n",
      "8                     000              000 job                  economy added   \n",
      "\n",
      "                          Word 15               Word 16         Word 17  \\\n",
      "0   president obama #americaleads             everybody           place   \n",
      "15                         future                planet             air   \n",
      "1                            real                  bold  action climate   \n",
      "17                           auto              majority          nearly   \n",
      "8               sector job growth  month private sector   month private   \n",
      "\n",
      "                  Word 18           Word 19  \n",
      "0                  longer              wish  \n",
      "15                   step             fight  \n",
      "1   action climate change  fighting climate  \n",
      "17               industry  working american  \n",
      "8         record breaking            streak  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"obama_tweets\", id_col=\"id\", text_col=\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg    Word 0          Word 1  \\\n",
      "0         0     5147  268.206925  0.052109  business            year   \n",
      "37       37     3014  145.498578  0.048274    school         primary   \n",
      "17       17     2944  151.974144  0.051622     small  small business   \n",
      "35       35     2616  108.472440  0.041465    income        increase   \n",
      "39       39     2513  231.857754  0.092263       100             200   \n",
      "\n",
      "             Word 2            Word 3         Word 4            Word 5  ...  \\\n",
      "0               old          year old           grow           product  ...   \n",
      "37   primary school             child      secondary  secondary school  ...   \n",
      "17            start  business selling       business              life  ...   \n",
      "35  increase income            family  family income        supplement  ...   \n",
      "39       requesting   requesting loan           lack         transport  ...   \n",
      "\n",
      "        Word 10               Word 11        Word 12  Word 13         Word 14  \\\n",
      "0      customer               started  business grow  married            good   \n",
      "37  school fees  child primary school        teacher  college  children child   \n",
      "17        dream        representative    start small    child           ready   \n",
      "35     generate                   way          build  support           needs   \n",
      "39       family                school             50      300           stock   \n",
      "\n",
      "    Word 15           Word 16        Word 17      Word 18          Word 19  \n",
      "0     began  started business       products          buy             help  \n",
      "37   attend          children           born  high school    attend school  \n",
      "17     feel            hoping  started small      poverty          someday  \n",
      "35     week          children          stock     earnings           rental  \n",
      "39  follows               buy            400   able repay  able repay loan  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"kiva_cleaned\", id_col=\"loan_id\", text_col=\"en_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/3YSXZq1EVj\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/6Ve3YJoStm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/Kgkz1Z3FwJ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg  Word 0       Word 1    Word 2  \\\n",
      "0         0      409  110.146960  0.269308   tesla  tesla model   vehicle   \n",
      "2         2      299   66.574882  0.222658  launch         live  tomorrow   \n",
      "6         6      285   74.675183  0.262018   model  tesla model    review   \n",
      "15       15      267   58.644813  0.219643  rocket            w    flight   \n",
      "13       13      241   62.322000  0.258598     car     best car  electric   \n",
      "\n",
      "                Word 3       Word 4      Word 5  ...          Word 10  \\\n",
      "0               charge        motor       owner  ...               la   \n",
      "2                   et      weather        cape  ...           window   \n",
      "6   @teslamotors model      model x     model 3  ...           safety   \n",
      "15              engine  grasshopper      thrust  ...  falcon 9 rocket   \n",
      "13           important        owner  production  ...            drive   \n",
      "\n",
      "        Word 11        Word 12    Word 13         Word 14     Word 15  \\\n",
      "0   tesla owner  tesla model x       free            took     battery   \n",
      "2     canaveral        attempt  satellite  cape canaveral  launch pad   \n",
      "6         motor          range          w          norway      series   \n",
      "15     9 rocket         aiming     firing            size       drone   \n",
      "13       tested           come        100            self    customer   \n",
      "\n",
      "     Word 16   Word 17       Word 18  Word 19  \n",
      "0       swap    faster  supercharger     open  \n",
      "2    tonight  vertical       advance     open  \n",
      "6    testing     cover          mile      kid  \n",
      "15    design  vertical     droneship  reentry  \n",
      "13  software  consumer           fun      min  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"elonmusk_tweets\", id_col=\"id\", text_col=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/ZQ0osiFEJQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/SmTkLPiBYD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"https://t.co/T5JBFXOz3F\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"http://t.co/PtViAyrO4A\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support       Weight       Avg            Word 0  \\\n",
      "1         1     9309  1740.069211  0.186923  @realdonaldtrump   \n",
      "12       12     4886   803.363908  0.164422             obama   \n",
      "4         4     4562   977.496570  0.214269             great   \n",
      "3         3     4295   836.511643  0.194764            donald   \n",
      "23       23     4195   687.396804  0.163861      @barackobama   \n",
      "\n",
      "                    Word 1       Word 2                   Word 3  \\\n",
      "1                      yes      awesome                     wait   \n",
      "12         president obama    obamacare                     iran   \n",
      "4   @realdonaldtrump great    great job                      guy   \n",
      "3             donald trump        trump  @realdonaldtrump donald   \n",
      "23                    cont  @mittromney                 campaign   \n",
      "\n",
      "                    Word 4                Word 5  ...             Word 10  \\\n",
      "1   @realdonaldtrump great  #celebrityapprentice  ...  #trumpforpresident   \n",
      "12                election                  care  ...               syria   \n",
      "4             great thanks             great guy  ...               honor   \n",
      "3   donald trump president       trump president  ...      @breitbartnews   \n",
      "23                    debt                   tax  ...                   1   \n",
      "\n",
      "            Word 11       Word 12        Word 13      Word 14  \\\n",
      "1       @megynkelly      watching       @foxnews  inspiration   \n",
      "12          charity      disaster    @mittromney        ebola   \n",
      "4   great interview  great people  america great    fantastic   \n",
      "3           million           gop          speak       energy   \n",
      "23              gas        called       election      america   \n",
      "\n",
      "                    Word 15                Word 16         Word 17  \\\n",
      "1   @realdonaldtrump thanks  @realdonaldtrump just             sir   \n",
      "12                  release               congress         amnesty   \n",
      "4            congratulation          country great          player   \n",
      "3              presidential               @newsmax  @newsmax media   \n",
      "23                  release               economic            jobs   \n",
      "\n",
      "                   Word 18   Word 19  \n",
      "1   @realdonaldtrump trump  favorite  \n",
      "12                   putin   million  \n",
      "4                wonderful      news  \n",
      "3                     plan       hit  \n",
      "23                 deficit   college  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"2017_trump_tweets\", id_col=\"id\", text_col=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topic ID  Support      Weight       Avg        Word 0       Word 1  \\\n",
      "0        0      649  146.779251  0.226162      @indiana          edc   \n",
      "3        3      591  108.506072  0.183597           new          job   \n",
      "1        1      554  107.555009  0.194143       indiana  edc indiana   \n",
      "4        4      501   99.913060  0.199427       hoosier  000 hoosier   \n",
      "2        2      451   96.989106  0.215053  @firstladyin          #in   \n",
      "\n",
      "                 Word 2       Word 3            Word 4             Word 5  \\\n",
      "0          @indiana edc     #indiana  #astatethatworks  #inregionalcities   \n",
      "3               new job     creating              plan          announced   \n",
      "1  @indiana edc indiana  agriculture           central      indiana state   \n",
      "4           hoosier job          000     hoosier state          workforce   \n",
      "2                  lady        #flcf           talking          christmas   \n",
      "\n",
      "   ...             Word 10 Word 11       Word 12               Word 13  \\\n",
      "0  ...               #tech   #indy   #indinjapan                 board   \n",
      "3  ...              create  center    plan today  announced plan today   \n",
      "1  ...                true    read    commission                number   \n",
      "4  ...         new hoosier    help  hoosier jobs             tradition   \n",
      "2  ...  @firstladyin thank     art       excited                 grant   \n",
      "\n",
      "         Word 14               Word 15           Word 16        Word 17  \\\n",
      "0           news             investing              indy            000   \n",
      "3       facility           new hoosier            adding      expansion   \n",
      "1  state indiana                 award             child         #icymi   \n",
      "4            job              continue  hoosier business           jobs   \n",
      "2       luncheon  @firstladyin excited             phone  lady luncheon   \n",
      "\n",
      "       Word 18             Word 19  \n",
      "0    edc #news  @indiana edc #news  \n",
      "3  hoosier job    expand operation  \n",
      "1         know             history  \n",
      "4        heart                hero  \n",
      "2          3rd  governor residence  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"pence_tweets\", id_col=\"id\", text_col=\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg    Word 0        Word 1    Word 2  \\\n",
      "1         1    10351  544.916602  0.052644   company  company said      said   \n",
      "12       12     7653  391.135532  0.051109         1             2       1 2   \n",
      "31       31     7298  500.927863  0.068639  mln dlrs           mln      dlrs   \n",
      "11       11     7210  410.434907  0.056926       pct          year     5 pct   \n",
      "4         4     6635  397.973836  0.059981     trade         japan  japanese   \n",
      "\n",
      "           Word 3  Word 4        Word 5  ...     Word 10  Word 11     Word 12  \\\n",
      "1     said reuter    plan  said company  ...    continue  expects  management   \n",
      "12            2 1     1 1       1 2 pct  ...       1 mln      1 3           7   \n",
      "31  mln dlrs year   4 mln   dlrs reuter  ...  5 mln dlrs    1 mln   dlrs year   \n",
      "11       pct year    rose         3 pct  ...       7 pct    1 pct      growth   \n",
      "4            said  tariff         world  ...      market   export     surplus   \n",
      "\n",
      "       Word 13       Word 14        Word 15  Word 16  Word 17      Word 18  \\\n",
      "1   subsidiary     statement  international     used  capital  shareholder   \n",
      "12       2 mln         wheat          1 000      1 8      dlr        april   \n",
      "31        1986       revenue          9 mln     year    8 mln        0 mln   \n",
      "11           2      increase              9     1986    4 pct         said   \n",
      "4      foreign  united state       minister  economy    state      meeting   \n",
      "\n",
      "      Word 19  \n",
      "1      annual  \n",
      "12        500  \n",
      "31      5 mln  \n",
      "11  inflation  \n",
      "4       tokyo  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"reutersCSV\", id_col=\"pid\", text_col=\"doc.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg  Word 0        Word 1  \\\n",
      "15       15     2486  186.897064  0.075180  temple  visit temple   \n",
      "0         0     2357  232.600945  0.098685    main   main temple   \n",
      "11       11     1914  163.995300  0.085682   place    good place   \n",
      "45       45     1257  112.293919  0.089335   visit  visit temple   \n",
      "32       32     1014  112.239750  0.110690    food    restaurant   \n",
      "\n",
      "             Word 2          Word 3        Word 4       Word 5  ...  \\\n",
      "15      temple good  krishna temple        modern          big  ...   \n",
      "0            temple             way          hall         shop  ...   \n",
      "11  beautiful place  peaceful place      pleasant  visit place  ...   \n",
      "45     temple visit     visit place  visit iskcon   good visit  ...   \n",
      "32            taste           tasty       variety   vegetarian  ...   \n",
      "\n",
      "         Word 10           Word 11        Word 12           Word 13  \\\n",
      "15    big temple  beautiful temple  modern temple  temple bangalore   \n",
      "0           exit             queue           free              long   \n",
      "11  place temple      spirituality     place good    commercialized   \n",
      "45           day              want            try              life   \n",
      "32     delicious      higher taste          stall          prasadam   \n",
      "\n",
      "            Word 14        Word 15          Word 16       Word 17  \\\n",
      "15          complex  iskcon temple  peaceful temple          kind   \n",
      "0             reach           book             need         enter   \n",
      "11        religious     love place          crowded  divine place   \n",
      "45        different        feeling            enjoy          make   \n",
      "32  vegetarian food   variety food           prasad       quality   \n",
      "\n",
      "           Word 18           Word 19  \n",
      "15  visited temple       best temple  \n",
      "0             area             thing  \n",
      "11           quite  place maintained  \n",
      "45       recommend           visitor  \n",
      "32            item               veg  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"ISKON_IMB767-XLS-ENG\", id_col=\"ID\", text_col=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support       Weight       Avg  Word 0        Word 1  \\\n",
      "0         0    25397  1318.337718  0.051909    like    taste like   \n",
      "9         9    21257  1150.331839  0.054115       1             2   \n",
      "67       67    20937   970.074548  0.046333     try       decided   \n",
      "12       12    20194  1230.732804  0.060945  flavor  great flavor   \n",
      "10       10    20068  1283.634622  0.063964   taste    taste like   \n",
      "\n",
      "         Word 2       Word 3       Word 4       Word 5  ... Word 10  \\\n",
      "0           doe  really like    just like   like taste  ...   stuff   \n",
      "9           fat            3            5      calorie  ...     1 2   \n",
      "67    different      thought         year  decided try  ...  wanted   \n",
      "12  good flavor       strong      texture  flavor good  ...     doe   \n",
      "10   taste good          doe  taste great    doe taste  ...     bad   \n",
      "\n",
      "        Word 11      Word 12   Word 13       Word 14      Word 15 Word 16  \\\n",
      "0   like coffee    feel like  probably          feel  tasted like    food   \n",
      "9         fiber          low     ounce        sodium         pack       6   \n",
      "67          way     year old       say         tried        going    want   \n",
      "12  like flavor  flavor just   flavors           bit   definitely     gum   \n",
      "10   taste just         real    bitter  taste better      texture     say   \n",
      "\n",
      "       Word 17       Word 18   Word 19  \n",
      "0         kind   flavor like  did like  \n",
      "9     calories            oz         8  \n",
      "67      review         loved     think  \n",
      "12     variety         peach     enjoy  \n",
      "10  artificial  coffee taste        ok  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"reviews_Grocery_and_Gourmet_Food_5_50000\", id_col=\"reviewID\", text_col=\"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (35,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support       Weight       Avg    Word 0             Word 1  \\\n",
      "0         0    29952  1934.085882  0.064573   patient  patient developed   \n",
      "2         2    28012  1481.919058  0.052903       day              2 day   \n",
      "31       31    24246  1301.990087  0.053699  benadryl            treated   \n",
      "13       13    22031  1667.537814  0.075691         b            subject   \n",
      "6         6    21919  1852.834846  0.084531        pt       pt developed   \n",
      "\n",
      "         Word 2            Word 3       Word 4       Word 5  ...    Word 10  \\\n",
      "0      reported  patient received     received  information  ...  physician   \n",
      "2         later         day later  day vaccine      vaccine  ...  following   \n",
      "31           po                er    urticaria        hives  ...        ice   \n",
      "13      engerix         engerix b    b vaccine    hepatitis  ...   received   \n",
      "6   pt received         developed     reported     received  ...  physician   \n",
      "\n",
      "           Word 11           Word 12   Word 13    Word 14     Word 15  \\\n",
      "0   female patient  reported patient      dose     number   recovered   \n",
      "2    day receiving            10 day     5 day      4 day    day post   \n",
      "31          keflex            motrin        rx  treatment  prednisone   \n",
      "13     unspecified        concurrent      dose   reported       event   \n",
      "6        recovered              info  pt given  female pt      office   \n",
      "\n",
      "            Word 16              Word 17    Word 18              Word 19  \n",
      "0              year           lot number        old             year old  \n",
      "2   day vaccination              started  receiving                    7  \n",
      "31       cellulitis                   im   compress                 took  \n",
      "13      recombinant  hepatitis b vaccine       date  vaccine recombinant  \n",
      "6              time          experienced     report                 seen  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"vaers2\", id_col=\"VAERS_ID\", text_col=\"SYMPTOM_TEXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"out\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"http://www.suntimes.com/2072121,desiree-rogers-quits-022610.article\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\bs4\\__init__.py:398: UserWarning: \"http://www.brookings.edu/—/media/Files/rc/reports/2010/08_arab_opinion_poll_telhami/08_arab_opinion_pollielhami.pdf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic ID  Support      Weight       Avg   Word 0  Word 1     Word 2  \\\n",
      "3         3     2608  531.334202  0.203732     said   obama  president   \n",
      "2         2     1690  208.505882  0.123376        h    2009         pm   \n",
      "8         8     1410  168.203102  0.119293       pm  office         00   \n",
      "58       58      869  107.421964  0.123616  hillary    2010   february   \n",
      "0         0      815  513.325876  0.629848      fyi      fw      issue   \n",
      "\n",
      "       Word 3     Word 4            Word 5  ...   Word 10  Word 11 Word 12  \\\n",
      "3         new      state            people  ...    policy       mr   house   \n",
      "2        pm h         fw            sunday  ...        12  tuesday       3   \n",
      "8   secretary         30  secretary office  ...      room    30 pm  arrive   \n",
      "58    october          7           january  ...  november        6    2012   \n",
      "0        news  discussed             pm fw  ...   meeting  article    note   \n",
      "\n",
      "   Word 13  Word 14   Word 15   Word 16    Word 17       Word 18  \\\n",
      "3    party  foreign    israel     woman        say       support   \n",
      "2        7   friday        11  december  september             6   \n",
      "8       en       45  en route     route     depart  pm secretary   \n",
      "58      12       27        13   clinton       best        friday   \n",
      "0       b6     team       way         c        jim          case   \n",
      "\n",
      "             Word 19  \n",
      "3            country  \n",
      "2               2010  \n",
      "8   state department  \n",
      "58                23  \n",
      "0               said  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "do_it(file_base=\"Hillary_Emails\", id_col=\"Id\", text_col=\"ExtractedBodyText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_sklearn_kernel",
   "language": "python",
   "name": "small_sklearn_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
