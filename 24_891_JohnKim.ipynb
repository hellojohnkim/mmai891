{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojohnkim/mmai891/blob/main/24_891_JohnKim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKmorPdno_n_"
      },
      "source": [
        "# MMAI 891: Individual Assignment\n",
        "\n",
        "Version 1: Updated February 4, 2024\n",
        "\n",
        "<font color='red'>\\# TODO: fill in the below</font>\n",
        "\n",
        "- John Kim\n",
        "- 20439250\n",
        "- MMAI 2024 891\n",
        "- Genius Makers by Cade Metz\n",
        "- Due April 21, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZFTCX4DqmRO"
      },
      "source": [
        "# Preliminaries: Inspect and Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "id": "MmxNeVzQHrMK",
        "outputId": "4edc88f1-3105-466f-ece4-cc2f4bfc4aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.9.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.0-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.0-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.10.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.2.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.6.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.16.3)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=851e16df4823dc51a24c4209ddf33c4716f56818ef6521c545c40bdd4de13091\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124735 sha256=cc530b19742fb2c646be0eed6fdbb57203bc99c4c9f64c43bb453753a065a5bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=3a9aeaecfc6af8a0faf01e819de817fdb2e059cb801d3f29fbd7ecca21615ab3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, jaraco.context, feedparser, autocommand, tempora, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-10.0.0 cherrypy-18.9.0 feedparser-6.0.11 jaraco.collections-5.0.0 jaraco.context-4.3.0 jaraco.functools-4.0.0 jaraco.text-3.12.0 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.0 sgmllib3k-1.0.0 tempora-5.5.1 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj34Jz-Do_oK"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQ_XOKyXTS6",
        "outputId": "36e8c4b2-3da6-4c6c-b703-626adf729ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-29 18:06:27.178941\n"
          ]
        }
      ],
      "source": [
        "print(datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfOMt1lErLhZ",
        "outputId": "1e770429-c3bd-440e-b1f2-eb40c372e3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n"
          ]
        }
      ],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aub2w1-arM5K",
        "outputId": "e72daf64-03e0-4877-b0c0-0bf84b463519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Y_n_8UrO9i",
        "outputId": "c454c8eb-d7ff-469f-bf60-7a77ce114564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n"
          ]
        }
      ],
      "source": [
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qyD7Jl0Gw1E"
      },
      "outputs": [],
      "source": [
        "# TODO: install any packages you need to here. For example:\n",
        "#pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from pattern.en import suggest"
      ],
      "metadata": {
        "id": "qckVJuPZ0ebE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj1NSQelo_oN"
      },
      "source": [
        "# Question 1: Sentiment Analysis via Shallow ML\n",
        "\n",
        "\n",
        "**Marking**\n",
        "\n",
        "The coding parts (i.e., 1.a, 1.b, 1.c4) will be marked based on:\n",
        "\n",
        "- *Correctness*. Code clearly and fully performs the task specified.\n",
        "- *Reproducibility*. Code is fully reproducible. I.e., you (and I) are able to run this Notebook again and again, from top to bottom, and get the same results each time.\n",
        "- *Style*. Code is organized. All parts commented with clear reasoning and rationale. No old code laying around. Code easy to follow.\n",
        "\n",
        "\n",
        "Parts 2 and 3 will be marked on:\n",
        "\n",
        "- *Quality*. Response is well-justified and convincing. Responses uses facts and data where possible.\n",
        "- *Style*. Response uses proper grammar, spelling, and punctuation. Response is clear and professional. Response is complete, but not overly-verbose. Response follows length guidelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60UNWiX8YmLi",
        "outputId": "d799673b-4745-4e96-822b-5bcaa4bfe5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2400 entries, 0 to 2399\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Sentence  2400 non-null   object\n",
            " 1   Polarity  2400 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 37.6+ KB\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# First, we'll read the provided labeled training data\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1b8MAiN-xBdk6scM-DnufkuijDZivZJqM\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ksKqf_fV6pIO"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# Next, we'll split it into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['Sentence']\n",
        "y = df['Polarity']\n",
        "\n",
        "# So that we can evaluate how well our model is performing, we split our training data\n",
        "# into training and validation.\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R04NzckZKbG2"
      },
      "source": [
        "## Part 1.a: Preprocessing and FE Pipeline\n",
        "\n",
        "Clean and preprocess the data (i.e., `X_train`) as you see necessary. Extract features from the text (i.e., vectorization using BOW and/or Bag of N-Grams and/or topics and/or lexical features).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qeavkicwo_oN"
      },
      "outputs": [],
      "source": [
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer from Hugging Face's transformers\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def preprocess_text(texts):\n",
        "    processed_texts = []\n",
        "    for text in texts:\n",
        "        # Tokenization\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "\n",
        "        # Spell checking using pattern.en and case normalization\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            suggestions = suggest(token)\n",
        "            # Choose the most likely correction (first suggestion) and lowercase it\n",
        "            corrected_token = suggestions[0][0].lower() if suggestions else token.lower()\n",
        "            corrected_tokens.append(corrected_token)\n",
        "\n",
        "        # Stop word removal\n",
        "        tokens_without_sw = [token for token in corrected_tokens if token not in stop_words]\n",
        "\n",
        "        # Reconstruct the text\n",
        "        processed_text = ' '.join(tokens_without_sw)\n",
        "        processed_texts.append(processed_text)\n",
        "\n",
        "    return processed_texts\n",
        "\n",
        "# Apply preprocessing to the training data\n",
        "X_train_processed = preprocess_text(X_train)\n",
        "\n",
        "# Initialize and fit the TF-IDF vectorizer with the preprocessed training data\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)"
      ],
      "metadata": {
        "id": "bW8dpTY5J11C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Perform various checks on the pre-processed data\n",
        "\n",
        "# 1. Inspect the shape of X_train_tfidf\n",
        "print(\"Shape of X_train_tfidf:\", X_train_tfidf.shape)\n",
        "\n",
        "# 2. View some non-zero elements from the first document's TF-IDF vector\n",
        "# Convert the sparse matrix row to a dense format and print non-zero elements\n",
        "first_doc_vector = X_train_tfidf[0].todense()\n",
        "print(\"Non-zero elements in the first document's TF-IDF vector:\", first_doc_vector[first_doc_vector > 0])\n",
        "\n",
        "# 3. Check the feature names (vocabulary)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"Some feature names:\", feature_names[:10])  # Print the first 10 feature names\n",
        "\n",
        "# 4. Sample document vector\n",
        "# For a more readable format, you can create a DataFrame for the first document's TF-IDF vector\n",
        "\n",
        "df_first_doc = pd.DataFrame(first_doc_vector, columns=feature_names)\n",
        "# Filter out zero values for clearer visibility\n",
        "df_nonzero = df_first_doc.loc[:, (df_first_doc != 0).any(axis=0)]\n",
        "print(\"TF-IDF values for non-zero features in the first document:\\n\", df_nonzero.T)\n"
      ],
      "metadata": {
        "id": "_xE0tiT3K2_z",
        "outputId": "e50e4552-1169-45f0-dd3f-5bfce410f224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_tfidf: (1800, 2956)\n",
            "Non-zero elements in the first document's TF-IDF vector: [[0.2861109  0.30179299 0.2861109  0.2861109  0.30179299 0.22136688\n",
            "  0.23585882 0.2861109  0.26635381 0.30179299 0.26635381 0.26635381\n",
            "  0.2861109 ]]\n",
            "Some feature names: ['00' '10' '100' '11' '12' '13' '15' '17' '18' '1979']\n",
            "TF-IDF values for non-zero features in the first document:\n",
            "                     0\n",
            "badly        0.286111\n",
            "concentrate  0.301793\n",
            "crack        0.286111\n",
            "energy       0.286111\n",
            "follow       0.301793\n",
            "found        0.221367\n",
            "hard         0.235859\n",
            "honestly     0.286111\n",
            "mean         0.266354\n",
            "miles        0.301793\n",
            "orders       0.266354\n",
            "predictable  0.266354\n",
            "youthful     0.286111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7FIkMnao_oO"
      },
      "source": [
        "## Part 1.b: Model Training/Tuning/Cross Validation\n",
        "\n",
        "Use your favorite shallow ML algorithm (such as decision trees, KNN, random forest, boosting variants) to train a classification model.  Don’t forget everything we’ve learned in the machine learning course: hyperparameter tuning, cross-validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing model that you can.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgek8ghwo_oP"
      },
      "outputs": [],
      "source": [
        "# TODO: insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjUgRW1N6ppS"
      },
      "source": [
        "## Part 1.c: Model Assessment\n",
        "\n",
        "Use your model to predict the sentiment of the testing data. Measure the performance (e.g., accuracy, AUC, F1-score) of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYlrWng9MqmL",
        "outputId": "e4c6a0f2-2150-4458-9d0b-df3e6a0675d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 600 entries, 0 to 599\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Sentence  600 non-null    object\n",
            " 1   Polarity  600 non-null    int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 9.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "test_df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1taoTluPBUMt9JkKAnlqDTrU49DJFpJGW\")\n",
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMm6r-wSOUY1"
      },
      "outputs": [],
      "source": [
        "# TODO: insert code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wUBcyrdM3__"
      },
      "source": [
        "## Part 2: Given the performance of your model, are you satisfied with the results? Explain.\n",
        "\n",
        "Keep your response to 1000 characters or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oCr-mTfNG-H"
      },
      "source": [
        "TODO: Insert answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz8dTvnJNKLL"
      },
      "source": [
        "## Part 3: Show five test instances in which your model was incorrect. Dive deep and find out why your model was wrong.\n",
        "\n",
        "Keep your response to 1000 characters or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DekZ7mulNTmr"
      },
      "source": [
        "TODO: Insert answer here. (Feel free to create new code cells if necessary.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2nBXXwBQaFk"
      },
      "source": [
        "# Question 2: Conceptual Understanding of the SOTA\n",
        "\n",
        "\n",
        "**Marking**\n",
        "\n",
        "The following questions will be marked on:\n",
        "\n",
        "- *Quality*. Response is well-justified and convincing. Responses uses facts and data where possible.\n",
        "- *Style*. Response uses proper grammar, spelling, and punctuation. Response is clear and professional. Response is complete, but not overly-verbose. Response follows length guidelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hmLwD6-Qs3U"
      },
      "source": [
        "## Part 1: What is transfer learning and fine-tuning in NLP? What advantages does it have over training from scratch?\n",
        "\n",
        "Keep your response to 1000 characters or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcXqPprFQyUM"
      },
      "source": [
        "TODO: Insert answer here. (Feel free to create new code cells if necessary.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oorJvilBQ2g1"
      },
      "source": [
        "## Part 2: What is a Large Language Model (LLM) and what are their strengths and weaknesses?\n",
        "\n",
        "Keep your response to 1000 characters or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNQn5LYaQ5qH"
      },
      "source": [
        "TODO: Insert answer here. (Feel free to create new code cells if necessary.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE2GL8VMNKHj"
      },
      "source": [
        "# Question 3 (Optional/Bonus): Sentiment Analysis via Deep ML\n",
        "\n",
        "This question is optional and is worth up to 5 extra credit marks.\n",
        "\n",
        "Use deep learning (e.g., RNNs and variants, CNNs and variants, and/or transformers) to build a model on the same dataset as Q1 and compare the results with the Shallow ML model.\n",
        "\n",
        "You may train your own deep ML model (using, e.g., the keras library) or fine-tune a pretrained deep ML model (using, e.g., the transformers library and the Huggingface ecoystem)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhM3_13LRdTI"
      },
      "outputs": [],
      "source": [
        "# TODO: Insert code here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}