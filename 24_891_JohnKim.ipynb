{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojohnkim/mmai891/blob/main/24_891_JohnKim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "JP3qOBTZQL-L",
        "outputId": "f3bb60b7-dbfe-43c9-f629-88d1b01b09d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2EaSyGvQPqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKmorPdno_n_"
      },
      "source": [
        "# MMAI 891: Individual Assignment\n",
        "\n",
        "Version 1: Updated February 4, 2024\n",
        "\n",
        "- John Kim\n",
        "- 20439250\n",
        "- MMAI 2024 891\n",
        "- Genius Makers by Cade Metz\n",
        "- Due April 21, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZFTCX4DqmRO"
      },
      "source": [
        "# Preliminaries: Inspect and Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj34Jz-Do_oK"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQ_XOKyXTS6",
        "outputId": "2e27ed87-72f3-43fd-c16a-8cf1fa5051c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-20 02:03:56.397737\n"
          ]
        }
      ],
      "source": [
        "print(datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfOMt1lErLhZ",
        "outputId": "22d8d560-18bb-4947-f6a8-da0a4e5e137a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n"
          ]
        }
      ],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aub2w1-arM5K",
        "outputId": "5b116d4a-0310-42c3-ff5d-c1f14e5bb97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Y_n_8UrO9i",
        "outputId": "d837c54f-ae85-48c6-a230-eb3ca5eafbfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n"
          ]
        }
      ],
      "source": [
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-qyD7Jl0Gw1E"
      },
      "outputs": [],
      "source": [
        "# TODO: install any packages you need to here. For example:\n",
        "#pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj1NSQelo_oN"
      },
      "source": [
        "# Question 1: Sentiment Analysis via Shallow ML\n",
        "\n",
        "\n",
        "**Marking**\n",
        "\n",
        "The coding parts (i.e., 1.a, 1.b, 1.c4) will be marked based on:\n",
        "\n",
        "- *Correctness*. Code clearly and fully performs the task specified.\n",
        "- *Reproducibility*. Code is fully reproducible. I.e., you (and I) are able to run this Notebook again and again, from top to bottom, and get the same results each time.\n",
        "- *Style*. Code is organized. All parts commented with clear reasoning and rationale. No old code laying around. Code easy to follow.\n",
        "\n",
        "\n",
        "Parts 2 and 3 will be marked on:\n",
        "\n",
        "- *Quality*. Response is well-justified and convincing. Responses uses facts and data where possible.\n",
        "- *Style*. Response uses proper grammar, spelling, and punctuation. Response is clear and professional. Response is complete, but not overly-verbose. Response follows length guidelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60UNWiX8YmLi",
        "outputId": "d6b5e20e-c6d2-40f3-8189-5ef51758b50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2400 entries, 0 to 2399\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Sentence  2400 non-null   object\n",
            " 1   Polarity  2400 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 37.6+ KB\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# First, we'll read the provided labeled training data\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1b8MAiN-xBdk6scM-DnufkuijDZivZJqM\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ksKqf_fV6pIO"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# Next, we'll split it into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['Sentence']\n",
        "y = df['Polarity']\n",
        "\n",
        "# So that we can evaluate how well our model is performing, we split our training data\n",
        "# into training and validation.\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R04NzckZKbG2"
      },
      "source": [
        "## Part 1.a: Preprocessing and FE Pipeline\n",
        "\n",
        "Clean and preprocess the data (i.e., `X_train`) as you see necessary. Extract features from the text (i.e., vectorization using BOW and/or Bag of N-Grams and/or topics and/or lexical features).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Import Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import unicodedata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import pos_tag\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Step 2 - Define a function to clean text\n",
        "def clean_text(text):\n",
        "    # Normalize text to Unicode form\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # Remove punctuation from each word\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # Remove non-alphabetic tokens\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # Filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Step 3 - Extract POS Features\n",
        "def extract_pos_features(text):\n",
        "    # Tokenize the text and get POS tags\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    # Count adjectives and adverbs\n",
        "    adj_adv_counts = {'ADJ': 0, 'ADV': 0}\n",
        "    for _, tag in pos_tags:\n",
        "        if tag.startswith('JJ'):  # Adjective\n",
        "            adj_adv_counts['ADJ'] += 1\n",
        "        elif tag.startswith('RB'):  # Adverb\n",
        "            adj_adv_counts['ADV'] += 1\n",
        "    return [adj_adv_counts['ADJ'], adj_adv_counts['ADV']]\n",
        "\n",
        "# Step 4 - Clean and Prepare Training Data\n",
        "X_train_cleaned = X_train.apply(clean_text)\n",
        "\n",
        "# Step 5 - Vectorization and Feature Engineering\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # Use TF-IDF and expand n-gram range to tri-grams\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_cleaned)\n",
        "\n",
        "# Extract POS features and convert to DataFrame\n",
        "pos_features = pd.DataFrame(X_train.apply(extract_pos_features).tolist(), columns=['Num_Adjectives', 'Num_Adverbs'])\n",
        "\n",
        "# Combine TF-IDF features with POS features for the training set\n",
        "X_train_features = pd.concat([pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out()), pos_features], axis=1)\n",
        "\n",
        "# Apply the same cleaning and feature extraction to the validation data\n",
        "X_val_cleaned = X_val.apply(clean_text)\n",
        "X_val_tfidf = vectorizer.transform(X_val_cleaned)\n",
        "\n",
        "# If you have other feature extraction processes like POS features, apply them here\n",
        "pos_features_val = pd.DataFrame(X_val.apply(extract_pos_features).tolist(), columns=['Num_Adjectives', 'Num_Adverbs'])\n",
        "X_val_features = pd.concat([pd.DataFrame(X_val_tfidf.toarray(), columns=vectorizer.get_feature_names_out()), pos_features_val], axis=1)\n",
        "\n",
        "# Ensure all feature names are strings to avoid sklearn processing errors\n",
        "X_train_features.columns = X_train_features.columns.astype(str)\n",
        "X_val_features.columns = X_val_features.columns.astype(str)\n"
      ],
      "metadata": {
        "id": "BWpgtWJjRgL9",
        "outputId": "0ec1a474-c1bf-48e4-ed99-6724f939817e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gc6eUEB8P7Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum up the counts of each term.\n",
        "sum_words = X_train_features.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "90jwt9l4SgLq",
        "outputId": "46b9cc79-af32-4855-e5ad-84eb7eb4ddc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'key of type tuple not found and not a MultiIndex'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f6c66f0ec003>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sum up the counts of each term.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f6c66f0ec003>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sum up the counts of each term.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key of type tuple not found and not a MultiIndex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# If key is contained, would have returned by now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'key of type tuple not found and not a MultiIndex'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the top 20 n-grams\n",
        "top_ngrams = words_freq[:20]\n",
        "words = [i[0] for i in top_ngrams]\n",
        "frequencies = [i[1] for i in top_ngrams]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(words, frequencies, color='skyblue')\n",
        "plt.xlabel('Frequencies')\n",
        "plt.ylabel('N-grams')\n",
        "plt.title('Top 20 N-grams')\n",
        "plt.gca().invert_yaxis()  # Invert axis to have the highest counts on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U2DE7HZOSlbt",
        "outputId": "50e5803a-8d24-4beb-e6da-4a560de93521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'words_freq' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9dc5473c544d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the top 20 n-grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtop_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_ngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'words_freq' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7FIkMnao_oO"
      },
      "source": [
        "## Part 1.b: Model Training/Tuning/Cross Validation\n",
        "\n",
        "Use your favorite shallow ML algorithm (such as decision trees, KNN, random forest, boosting variants) to train a classification model.  Don’t forget everything we’ve learned in the machine learning course: hyperparameter tuning, cross-validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing model that you can.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cgek8ghwo_oP",
        "outputId": "a4d15acd-a3cf-4d1a-f590-21454c71bdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'optuna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-538a5f1fca33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import joblib\n",
        "\n",
        "# Prepare classifiers\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=100)\n",
        "dtree = RandomForestClassifier(random_state=42, n_estimators=50, max_depth=5)\n",
        "bagging = BaggingClassifier(estimator=dtree, n_estimators=5, random_state=42)\n",
        "boosting = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "xgboost = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "# Define an objective function to be minimized.\n",
        "def objective(trial):\n",
        "    # Hyperparameter setting\n",
        "    xgb_params = {\n",
        "        'max_depth': trial.suggest_int('xgb__max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('xgb__learning_rate', 0.01, 0.1),\n",
        "        'n_estimators': trial.suggest_int('xgb__n_estimators', 50, 150),\n",
        "        'scale_pos_weight': trial.suggest_categorical('xgb__scale_pos_weight', [1, sum(y_train == 0) / sum(y_train == 1)])\n",
        "    }\n",
        "\n",
        "    xgb = XGBClassifier(**xgb_params, random_state=42)\n",
        "\n",
        "    # Stack classifiers including the tuned XGBoost\n",
        "    estimators = [\n",
        "        ('rf', dtree),\n",
        "        ('bg', bagging),\n",
        "        ('adb', boosting),\n",
        "        ('xgb', xgb)  # Use the tuned XGBoost\n",
        "    ]\n",
        "    stacking_clf = StackingClassifier(estimators=estimators, final_estimator=log_reg, stack_method='auto', n_jobs=-1)\n",
        "\n",
        "    # Cross-validation scores\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(stacking_clf, X_train_features, y_train, cv=kf, scoring='accuracy')\n",
        "\n",
        "    return scores.mean()\n",
        "\n",
        "# Create a study object and optimize the objective function.\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Best trial\n",
        "trial = study.best_trial\n",
        "print(f'Accuracy: {trial.value}')\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))\n",
        "\n",
        "# Rebuild the model with the best parameters\n",
        "xgb_best = XGBClassifier(**trial.params, objective='binary:logistic', random_state=42)\n",
        "stacking_clf_best = StackingClassifier(estimators=[\n",
        "        ('rf', dtree),\n",
        "        ('bg', bagging),\n",
        "        ('adb', boosting),\n",
        "        ('xgb', xgb_best)\n",
        "    ], final_estimator=log_reg, stack_method='auto', n_jobs=-1)\n",
        "\n",
        "# Fit the best model\n",
        "stacking_clf_best.fit(X_train_features, y_train)\n",
        "\n",
        "\n",
        "# Save the model to disk\n",
        "joblib.dump(stacking_clf_best, 'stacking_clf_best.joblib')\n",
        "\n",
        "# Validate model\n",
        "y_pred = stacking_clf_best.predict(X_val_features)\n",
        "y_proba = stacking_clf_best.predict_proba(X_val_features)[:, 1]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
        "roc_auc = roc_auc_score(y_val, y_proba)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"ROC-AUC: {roc_auc}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the validation set\n",
        "X_val_cleaned = X_val.apply(clean_text)\n",
        "X_val_features = vectorizer.transform(X_val_cleaned)  # Make sure to transform the validation set\n",
        "y_pred = grid_search.predict(X_val_features)\n",
        "\n",
        "# Print the classification report and confusion matrix\n",
        "print(classification_report(y_val, y_pred))\n",
        "print(confusion_matrix(y_val, y_pred))\n"
      ],
      "metadata": {
        "id": "iZP8Pm3pTjSk",
        "outputId": "079dbaaf-f8f5-47b6-daff-8f17c86fd24b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.91      0.77       323\n",
            "           1       0.82      0.46      0.59       277\n",
            "\n",
            "    accuracy                           0.70       600\n",
            "   macro avg       0.74      0.69      0.68       600\n",
            "weighted avg       0.73      0.70      0.69       600\n",
            "\n",
            "[[294  29]\n",
            " [149 128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "print(\"Best cross-validated score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "mWQYyaX3tHBB",
        "outputId": "f9ea0ffd-edf1-49eb-fb33-93c188099b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance'}\n",
            "Best cross-validated score: 0.6377777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjUgRW1N6ppS"
      },
      "source": [
        "## Part 1.c: Model Assessment\n",
        "\n",
        "Use your model to predict the sentiment of the testing data. Measure the performance (e.g., accuracy, AUC, F1-score) of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYlrWng9MqmL",
        "outputId": "d756d0ff-6fd5-4ce3-9984-f8e2002f5040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 600 entries, 0 to 599\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Sentence  600 non-null    object\n",
            " 1   Polarity  600 non-null    int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 9.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "test_df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1taoTluPBUMt9JkKAnlqDTrU49DJFpJGW\")\n",
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `clean_text` function has been defined as shown earlier\n",
        "test_df['cleaned_text'] = test_df['Sentence'].apply(clean_text)\n",
        "\n",
        "# Vectorize the test data using the same vectorizer fitted on the training data\n",
        "X_test_features = vectorizer.transform(test_df['cleaned_text'])\n"
      ],
      "metadata": {
        "id": "LXqwPW4fuRzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = grid_search.predict(X_test_features)"
      ],
      "metadata": {
        "id": "5Yo1V2JVuTpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMm6r-wSOUY1",
        "outputId": "fd6762a6-119e-4ded-b0bb-031e79aa9f44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5783333333333334\n",
            "F1-Score: 0.5181348189449905\n",
            "AUC: 0.7011165410604356\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.95      0.68       287\n",
            "           1       0.85      0.23      0.37       313\n",
            "\n",
            "    accuracy                           0.58       600\n",
            "   macro avg       0.69      0.59      0.53       600\n",
            "weighted avg       0.70      0.58      0.52       600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(test_df['Polarity'], y_test_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(test_df['Polarity'], y_test_pred, average='weighted')  # 'weighted' in case of class imbalance\n",
        "print(f\"F1-Score: {f1}\")\n",
        "\n",
        "# AUC can only be computed for binary classification or by one-vs-rest for multiclass classification\n",
        "if len(np.unique(y_train)) == 2:\n",
        "    # Predicting probabilities for AUC\n",
        "    y_test_prob = grid_search.predict_proba(X_test_features)[:, 1]\n",
        "    auc = roc_auc_score(test_df['Polarity'], y_test_prob)\n",
        "    print(f\"AUC: {auc}\")\n",
        "else:\n",
        "    print(\"AUC is not calculated because it's not a binary classification.\")\n",
        "\n",
        "# Detailed classification report\n",
        "report = classification_report(test_df['Polarity'], y_test_pred)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wUBcyrdM3__"
      },
      "source": [
        "## Part 2: Given the performance of your model, are you satisfied with the results? Explain.\n",
        "\n",
        "Keep your response to 1000 characters or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oCr-mTfNG-H"
      },
      "source": [
        "TODO: Insert answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz8dTvnJNKLL"
      },
      "source": [
        "## Part 3: Show five test instances in which your model was incorrect. Dive deep and find out why your model was wrong.\n",
        "\n",
        "Keep your response to 1000 characters or less."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Step 1: Identify Misclassified Instances\n",
        "\n",
        "# Add predictions to the test dataframe\n",
        "test_df['predicted_polarity'] = y_test_pred\n",
        "\n",
        "# Find instances where the predicted polarity does not match the actual polarity\n",
        "misclassified = test_df[test_df['Polarity'] != test_df['predicted_polarity']]\n"
      ],
      "metadata": {
        "id": "Pu4HgEBZuqMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Display Five Misclassified Instances\n",
        "# Randomly sample 5 misclassified instances for analysis\n",
        "misclassified_samples = misclassified.sample(n=5, random_state=42)\n",
        "print(misclassified_samples)"
      ],
      "metadata": {
        "id": "iRFiJreXuv5g",
        "outputId": "2895641c-b6e6-4668-c75a-b3414337c673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Sentence  Polarity  \\\n",
            "536  The two main characters may be two of the most...         1   \n",
            "10   A world better than 95% of the garbage in the ...         1   \n",
            "242  There's barely a boring moment in the film and...         1   \n",
            "532  The interplay between Martin and Emilio contai...         1   \n",
            "329  The movie is full of wonderful dancing (hence ...         1   \n",
            "\n",
            "                                          cleaned_text  predicted_polarity  \n",
            "536  two main character may two believable child ev...                   0  \n",
            "10                  world better garbage theatre today                   0  \n",
            "242     barely boring moment film plenty humorous part                   0  \n",
            "532  interplay martin emilio contains wonderful che...                   0  \n",
            "329           movie full wonderful dancing hence title                   0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2nBXXwBQaFk"
      },
      "source": [
        "# Question 2: Conceptual Understanding of the SOTA\n",
        "\n",
        "\n",
        "**Marking**\n",
        "\n",
        "The following questions will be marked on:\n",
        "\n",
        "- *Quality*. Response is well-justified and convincing. Responses uses facts and data where possible.\n",
        "- *Style*. Response uses proper grammar, spelling, and punctuation. Response is clear and professional. Response is complete, but not overly-verbose. Response follows length guidelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hmLwD6-Qs3U"
      },
      "source": [
        "## Part 1: What is transfer learning and fine-tuning in NLP? What advantages does it have over training from scratch?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the domain of NLP, transfer learning is a strategic approach where we utilize a model pre-trained on extensive datasets and tailor it to our specific needs. Fine-tuning further refines this process by adjusting the model to the intricacies of our targeted task. This methodology stands out for its efficiency, requiring significantly less time and data to achieve high-quality outcomes compared to traditional models built from the ground up. It leverages the foundational language understanding already embedded in the pre-trained model, providing a robust starting point and facilitating enhanced performance, even in data-scarce scenarios. This approach not only streamlines the development process but also amplifies the potential for innovative solutions in the field of NLP.\n",
        "\n",
        "characters: 789"
      ],
      "metadata": {
        "id": "SyxRytnhL4sp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oorJvilBQ2g1"
      },
      "source": [
        "## Part 2: What is a Large Language Model (LLM) and what are their strengths and weaknesses?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Large Language Model (LLM) like GPT excels in generating human-like text, understanding context, and performing diverse tasks in natural language. Strengths include their versatility across various applications, deep language understanding, and adaptability to specific domains via fine-tuning. However, LLMs face challenges like data bias, which can perpetuate ethical issues, and interpretability problems, complicating error analysis. They also demand significant computational resources, impacting accessibility and the environment. Despite their proficiency, LLMs can sometimes produce inaccurate or nonsensical outputs, especially in complex situations."
      ],
      "metadata": {
        "id": "b__YSyckMaXu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE2GL8VMNKHj"
      },
      "source": [
        "# Question 3 (Optional/Bonus): Sentiment Analysis via Deep ML\n",
        "\n",
        "This question is optional and is worth up to 5 extra credit marks.\n",
        "\n",
        "Use deep learning (e.g., RNNs and variants, CNNs and variants, and/or transformers) to build a model on the same dataset as Q1 and compare the results with the Shallow ML model.\n",
        "\n",
        "You may train your own deep ML model (using, e.g., the keras library) or fine-tune a pretrained deep ML model (using, e.g., the transformers library and the Huggingface ecoystem)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# First, we'll read the provided labeled training data\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1b8MAiN-xBdk6scM-DnufkuijDZivZJqM\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "mHAA4Zgtwukv",
        "outputId": "1d1e3996-eca2-4c6f-b721-5fd2bcd97ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2400 entries, 0 to 2399\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Sentence  2400 non-null   object\n",
            " 1   Polarity  2400 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 37.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# Next, we'll split it into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['Sentence']\n",
        "y = df['Polarity']\n",
        "\n",
        "# So that we can evaluate how well our model is performing, we split our training data\n",
        "# into training and validation.\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "tMv7CLV0wxU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Function to tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['Sentence'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Convert the Pandas dataframes to Hugging Face dataset format\n",
        "train_dataset = Dataset.from_pandas(df.iloc[X_train.index])\n",
        "val_dataset = Dataset.from_pandas(df.iloc[X_val.index])\n",
        "\n",
        "# Apply the tokenize function and remove original columns\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['Sentence', 'Polarity'])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['Sentence', 'Polarity'])\n",
        "\n",
        "# Add a column for labels which is required by Hugging Face Trainer\n",
        "train_dataset = train_dataset.add_column(\"labels\", y_train.tolist())\n",
        "val_dataset = val_dataset.add_column(\"labels\", y_val.tolist())\n"
      ],
      "metadata": {
        "id": "gi7ZzGmXwzpM",
        "outputId": "6ee3cd5e-16d9-4a7f-9abb-b2edf4e0b236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "dddaebad74aa4b9aa3ff6f88ec038306",
            "564c9bd7738c483da45dc4538422a8d6",
            "124b3198743c482ebbdb0fa77c5372b6",
            "32fd9bbcb02044cc8501817bc013fc95",
            "c1f459dd8cf4448e9167715d24ea544e",
            "a48fb9f67ece482dac59e4ec19e2690c",
            "7e5d0b49a0a248768da1265d1da369fc",
            "340db8350c0e47c997d6f28b657bc7ef",
            "adcdd0db53c64ea09986d374a76347af",
            "0c04c4af300c44be8584d88f6bc94358",
            "98d5e751803a454389390c796afed385",
            "c1ca79aa7acc4ee7ac7c2dfc31f9bf60",
            "427b7773b8c64b4497744214e3656482",
            "b77be6a6baf742d4b6ecc8d75beef9a8",
            "a48c8cd7579c4964835ebbdc2589d558",
            "219971637c7d4b939ca28d65390ac8e3",
            "7dc56ddecc784512a7339261825a3f99",
            "1c5c0a919b864305a90e1a34746f16c0",
            "bf275aeb3ceb4d5d9c1e69d0eab124af",
            "523664f7d54446ea82d01518f9a2c70a",
            "7c4af3e97eed4a7da4a79679605203a2",
            "79bfe9d863a04831ba046f75a4c17117"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dddaebad74aa4b9aa3ff6f88ec038306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1ca79aa7acc4ee7ac7c2dfc31f9bf60"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n"
      ],
      "metadata": {
        "id": "BBl5GFBTxL3a",
        "outputId": "a98af0c1-44cb-41af-e401-e4f284a4ca80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LTHCJ-eIxQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "g0ZhzaXqyVg_",
        "outputId": "5c2610fe-9795-471b-e2c6-f2415fadc07b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 03:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.693200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.679200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.693200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.684300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.685900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.570500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.474600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.333700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.229500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.217200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.243100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.178200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.253100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.166500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.237200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.345000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.158700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.174800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.118300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.083800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.074500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.114600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.090900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.255200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.124400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.098000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.065700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=339, training_loss=0.32095893294410366, metrics={'train_runtime': 239.6967, 'train_samples_per_second': 22.528, 'train_steps_per_second': 1.414, 'total_flos': 715323952742400.0, 'train_loss': 0.32095893294410366, 'epoch': 3.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Evaluation\n",
        "#Evaluate the model on the validation set to see how well it performs.\n",
        "\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "obEuPhixyXN2",
        "outputId": "27bd1582-73ac-4778-d2d7-deecdd7f3286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2601062059402466, 'eval_runtime': 10.6433, 'eval_samples_per_second': 56.374, 'eval_steps_per_second': 0.94, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7: Save the Model\n",
        "#After training, you may want to save the model and the tokenizer for later use.\n",
        "\n",
        "model_path = \"./sentiment-distilbert-model\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "vljq7w0gyzep",
        "outputId": "1c687429-13e8-4b15-c18d-18c9669fe1ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./sentiment-distilbert-model/tokenizer_config.json',\n",
              " './sentiment-distilbert-model/special_tokens_map.json',\n",
              " './sentiment-distilbert-model/vocab.txt',\n",
              " './sentiment-distilbert-model/added_tokens.json',\n",
              " './sentiment-distilbert-model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "predictions = trainer.predict(val_dataset)"
      ],
      "metadata": {
        "id": "3GSV4JigAMWO",
        "outputId": "1cacc1ea-acc1-40e3-9467-f26477a74e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
        "\n",
        "# Extract logits\n",
        "logits = predictions.predictions\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "\n",
        "# Extract predicted labels\n",
        "predicted_labels = np.argmax(logits, axis=1)\n",
        "\n",
        "# Extract True labels\n",
        "true_labels = np.array(val_dataset[\"labels\"])\n"
      ],
      "metadata": {
        "id": "vXkwsOn7ANlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# AUC - This metric is only meaningful for binary classification tasks\n",
        "if len(np.unique(true_labels)) == 2:\n",
        "    auc = roc_auc_score(true_labels, probabilities[:, 1])  # Probabilities for the positive class\n",
        "    print(f\"AUC: {auc}\")\n",
        "else:\n",
        "    print(\"AUC is not applicable for non-binary classification tasks.\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')  # Use 'weighted' to handle imbalanced classes\n",
        "print(f\"F1-Score: {f1}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "JBkIoHISBBM6",
        "outputId": "5aabaa66-6da6-4508-c2e3-5c72ebab002a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9216666666666666\n",
            "AUC: 0.9785628862983536\n",
            "F1-Score: 0.9217507258495342\n",
            "Confusion Matrix:\n",
            "[[294  29]\n",
            " [ 18 259]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "V-six-lZB2yu",
        "outputId": "c32d0a73-9d17-4231-bad0-ef0b67753c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.93       323\n",
            "           1       0.90      0.94      0.92       277\n",
            "\n",
            "    accuracy                           0.92       600\n",
            "   macro avg       0.92      0.92      0.92       600\n",
            "weighted avg       0.92      0.92      0.92       600\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dddaebad74aa4b9aa3ff6f88ec038306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_564c9bd7738c483da45dc4538422a8d6",
              "IPY_MODEL_124b3198743c482ebbdb0fa77c5372b6",
              "IPY_MODEL_32fd9bbcb02044cc8501817bc013fc95"
            ],
            "layout": "IPY_MODEL_c1f459dd8cf4448e9167715d24ea544e"
          }
        },
        "564c9bd7738c483da45dc4538422a8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a48fb9f67ece482dac59e4ec19e2690c",
            "placeholder": "​",
            "style": "IPY_MODEL_7e5d0b49a0a248768da1265d1da369fc",
            "value": "Map: 100%"
          }
        },
        "124b3198743c482ebbdb0fa77c5372b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_340db8350c0e47c997d6f28b657bc7ef",
            "max": 1800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adcdd0db53c64ea09986d374a76347af",
            "value": 1800
          }
        },
        "32fd9bbcb02044cc8501817bc013fc95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c04c4af300c44be8584d88f6bc94358",
            "placeholder": "​",
            "style": "IPY_MODEL_98d5e751803a454389390c796afed385",
            "value": " 1800/1800 [00:00&lt;00:00, 4376.08 examples/s]"
          }
        },
        "c1f459dd8cf4448e9167715d24ea544e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a48fb9f67ece482dac59e4ec19e2690c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5d0b49a0a248768da1265d1da369fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "340db8350c0e47c997d6f28b657bc7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcdd0db53c64ea09986d374a76347af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c04c4af300c44be8584d88f6bc94358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d5e751803a454389390c796afed385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1ca79aa7acc4ee7ac7c2dfc31f9bf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_427b7773b8c64b4497744214e3656482",
              "IPY_MODEL_b77be6a6baf742d4b6ecc8d75beef9a8",
              "IPY_MODEL_a48c8cd7579c4964835ebbdc2589d558"
            ],
            "layout": "IPY_MODEL_219971637c7d4b939ca28d65390ac8e3"
          }
        },
        "427b7773b8c64b4497744214e3656482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dc56ddecc784512a7339261825a3f99",
            "placeholder": "​",
            "style": "IPY_MODEL_1c5c0a919b864305a90e1a34746f16c0",
            "value": "Map: 100%"
          }
        },
        "b77be6a6baf742d4b6ecc8d75beef9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf275aeb3ceb4d5d9c1e69d0eab124af",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_523664f7d54446ea82d01518f9a2c70a",
            "value": 600
          }
        },
        "a48c8cd7579c4964835ebbdc2589d558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c4af3e97eed4a7da4a79679605203a2",
            "placeholder": "​",
            "style": "IPY_MODEL_79bfe9d863a04831ba046f75a4c17117",
            "value": " 600/600 [00:00&lt;00:00, 5053.54 examples/s]"
          }
        },
        "219971637c7d4b939ca28d65390ac8e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc56ddecc784512a7339261825a3f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5c0a919b864305a90e1a34746f16c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf275aeb3ceb4d5d9c1e69d0eab124af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523664f7d54446ea82d01518f9a2c70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c4af3e97eed4a7da4a79679605203a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79bfe9d863a04831ba046f75a4c17117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}